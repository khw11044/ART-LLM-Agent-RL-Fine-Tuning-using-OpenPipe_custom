{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emg-y6MMIX6D"
      },
      "source": [
        "## LangGraphë¥¼ ì´ìš©í•´ì„œ email search agentë¥¼ ê°•í™”í•™ìŠµìœ¼ë¡œ í›ˆë ¨ì‹œí‚¤ê¸°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Email Search Agent with LangGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” LangGraphì™€ ARTë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ARTâ€¢E ì—ì´ì „íŠ¸ë¥¼ ì²˜ìŒë¶€í„° ì§ì ‘ í•™ìŠµì‹œì¼œ ë³¼ ê²ƒì…ë‹ˆë‹¤! \n",
        "\n",
        "ì´ êµ¬í˜„ì€ LangGraphì˜ agent frameworkì™€ ARTì˜ training ê¸°ëŠ¥ì„ í†µí•©í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
        "\n",
        "Qwen2.5 - 7B base modelë¡œ ì‹œì‘í•˜ì—¬, LangGraphì˜ ReAct agent patternì„ ì‚¬ìš©í•˜ì—¬, ìš°ë¦¬ëŠ” ì´ê²ƒì„ í›ˆë ¨ ì‹œì¼œ ëª¨ë¸ì´ ì´ë©”ì¼ì„ ê²€ìƒ‰í•˜ê³  ì§ˆë¬¸ì— ëŒ€ë‹µì„ í•˜ê²Œ í•©ë‹ˆë‹¤.\n",
        "\n",
        "ìš°ë¦¬ëŠ” agentic environmentë¥¼ êµ¬ì¶•í•  ê²ƒì´ê³ , LangGraphë¥¼ ì´ìš©í•˜ì—¬ rolloutì„ ì •ì˜í•˜ê³ , training loopë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
        "\n",
        "ìš°ë¦¬ëŠ” ë˜í•œ agnetì˜ answersë“¤ì˜ í’ˆì§ˆì„ íŒë‹¨í•˜ê¸° ìœ„í•œ RULERë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í•  ê²ƒì¸ì§€ ë°°ìš¸ ê²ƒì…ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RULER\n",
        "\n",
        "RULERëŠ” agentì˜ answers í’ˆì§ˆì„ í‰ê°€í•˜ê³ , agentê°€ ìµœìƒì˜ ì™„ì„±ì„ ë” ì˜ ìƒì‚°í•˜ë„ë¡ í›ˆë ¨ì‹œí‚¤ëŠ” robustí•œ ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
        "\n",
        "RULERì— ëŒ€í•´ì„œ ë” ì•Œì•„ë³´ê³  ì‹¶ë‹¤ë©´ ë‹¤ìŒ ë§í¬ì— ì ‘ì†í•˜ì„¸ìš”:\n",
        "[RULER](https://art.openpipe.ai/fundamentals/ruler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QBijDl7iIX6F"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.11.13 environment at: /home/khw/miniconda3/envs/openpipe\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m6 packages\u001b[0m \u001b[2min 82ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#@title ğŸ’¿ Installation\n",
        "\n",
        "# Portions adapted from Unsloth Notebooks (https://github.com/unslothai/notebooks)\n",
        "# Copyright (c) Unsloth contributors.\n",
        "# License: GNU LGPL v3.0.\n",
        "# Modifications by OpenPipe:\n",
        "# - switched to uv\n",
        "# - changed vllm/triton pinning logic\n",
        "# - added litellm/protobuf pins\n",
        "# See /licenses/LGPL-3.0.txt and /licenses/GPL-3.0.txt for full text.\n",
        "\n",
        "# %%capture\n",
        "import os\n",
        "\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !uv pip install openpipe-art[backend,langgraph]==0.4.11 langchain-core langgraph langchain_openai tenacity datasets --prerelease allow --no-cache-dir\n",
        "else:\n",
        "    try:\n",
        "        import numpy\n",
        "\n",
        "        get_numpy = f\"numpy=={numpy.__version__}\"\n",
        "    except:\n",
        "        get_numpy = \"numpy\"\n",
        "    try:\n",
        "        import subprocess\n",
        "\n",
        "        is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except:\n",
        "        is_t4 = False\n",
        "    get_vllm, get_triton = (\n",
        "        (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm\", \"triton\")\n",
        "    )\n",
        "    !uv pip install --upgrade \\\n",
        "        openpipe-art[backend,langgraph]==0.4.11 langchain-core langgraph langchain_openai tenacity datasets protobuf==5.29.5 {get_vllm} {get_numpy} --prerelease allow --no-cache-dir\n",
        "    !uv pip install -qqq {get_triton}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOMCNkBVIX6G"
      },
      "source": [
        "<a name=\"Environment-Variables\"></a>\n",
        "\n",
        "### Environment Variables\n",
        "\n",
        "**OpenAI (used for RULER judge model)**\n",
        "\n",
        "ìš°ë¦¬ì˜ RULER ë³´ìƒ í•¨ìˆ˜ëŠ” third-party modelsì„ ì¿¼ë¦¬í•˜ì—¬ ì—ì´ì „íŠ¸ì˜ performance í’ˆì§ˆì„ íŒë‹¨í•©ë‹ˆë‹¤. LiteLLMì—ì„œ ì§€ì›í•˜ëŠ” ëª¨ë“  ëª¨ë¸ì´ ì‘ë™í•©ë‹ˆë‹¤. ì´ ì˜ˆì œì—ì„œëŠ” OpenAIì˜ o4-mini ëª¨ë¸ì„ ì‚¬ìš©í•˜ë¯€ë¡œ `OPENAI_API_KEY` í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "\n",
        "**Weights & Biases (optional)**\n",
        "\n",
        "ë‚˜ì¤‘ì— ë…¸íŠ¸ë¶ì—ì„œ Weights & Biasesì— ë©”íŠ¸ë¦­ì„ ìë™ìœ¼ë¡œ ê¸°ë¡í•˜ê³  Weaveì— ì±„íŒ… ì™„ë£Œë¥¼ ìë™ìœ¼ë¡œ ê¸°ë¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ ê²ƒì…ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ” Weights & Biases API í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ë¡œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vx9dP2GNIX6G"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Required\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    raise ValueError(\n",
        "        \"OPENAI_API_KEY is required for RULER functionality when using openai/o4-mini.\"\n",
        "    )\n",
        "\n",
        "# Optional\n",
        "# os.environ[\"WANDB_API_KEY\"] = \"YOUR_API_KEY\"\n",
        "\n",
        "if not os.environ.get(\"WANDB_API_KEY\"):\n",
        "    print(\"WANDB_API_KEY is not set. We'll skip logging metrics to Weights & Biases.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9QaTu1CIX6G"
      },
      "source": [
        "<a name=\"Environment\"></a>\n",
        "\n",
        "### Email Search Environment\n",
        "\n",
        "ARTëŠ” ì—ì´ì „íŠ¸ê°€ í™˜ê²½ê³¼ ìƒí˜¸ ì‘ìš©í•˜ì—¬ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì´ ì˜ˆì‹œì—ì„œëŠ” **LangGraphì˜ ë„êµ¬ í†µí•©**ì„ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ê°€ __ì´ë©”ì¼ì„ ê²€ìƒ‰í•˜ê³  ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìˆëŠ” í™˜ê²½__ ì„ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì´ agentê°€ ì ‘ê·¼ í•  ìˆ˜ ìˆëŠ” 3ê°€ì§€ ë„êµ¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
        "1. `search_inbox` - Search for emails by keywords\n",
        "2. `read_email` - Read a specific email by message ID\n",
        "3. `return_final_answer` - Return the final answer with source email IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9Xvmi-yIX6H",
        "outputId": "d43031fc-4ba8-4d1b-a941-b80462ed519c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/khw/miniconda3/envs/openpipe/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading train scenarios from Hugging Face...\n",
            "Loaded 50 scenarios.\n",
            "Email search environment created with full Enron dataset!\n",
            "Database contains the complete email dataset, loaded 50 training scenarios.\n",
            "\n",
            "Sample scenario\n",
            "id: 3296\n",
            "question: Who can I contact for Power Operations when Sally is in London?\n",
            "answer: Stacey White (x31870) and Leslie Reeves (x37962).\n",
            "message_ids: ['<6033065.1075856098960.JavaMail.evans@thyme>']\n",
            "how_realistic: 0.699999988079071\n",
            "inbox_address: louise.kitchen@enron.com\n",
            "query_date: 2001-01-25\n",
            "split: train\n"
          ]
        }
      ],
      "source": [
        "#@title Email Search Code\n",
        "\n",
        "import os\n",
        "import random\n",
        "import sqlite3\n",
        "from dataclasses import asdict, dataclass\n",
        "from datetime import datetime\n",
        "from textwrap import dedent\n",
        "from typing import List, Literal, Optional\n",
        "\n",
        "from datasets import Dataset, Features, Sequence, Value, load_dataset\n",
        "from pydantic import BaseModel, Field\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Email and Scenario data models\n",
        "class Email(BaseModel):\n",
        "    message_id: str\n",
        "    date: str  # ISO 8601 string 'YYYY-MM-DD HH:MM:SS'\n",
        "    subject: Optional[str] = None\n",
        "    from_address: Optional[str] = None\n",
        "    to_addresses: List[str] = []  # Populated from recipients table\n",
        "    cc_addresses: List[str] = []  # Populated from recipients table\n",
        "    bcc_addresses: List[str] = []  # Populated from recipients table\n",
        "    body: Optional[str] = None\n",
        "    file_name: Optional[str] = None\n",
        "\n",
        "\n",
        "class Scenario(BaseModel):\n",
        "    id: int\n",
        "    question: str\n",
        "    answer: str\n",
        "    message_ids: List[str]  # message_ids (strings) of referenced emails\n",
        "    how_realistic: float\n",
        "    inbox_address: str\n",
        "    query_date: str\n",
        "    split: Literal[\"train\", \"test\"]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SearchResult:\n",
        "    message_id: str\n",
        "    snippet: str\n",
        "\n",
        "\n",
        "class FinalAnswer(BaseModel):\n",
        "    answer: str\n",
        "    source_ids: list[str]\n",
        "\n",
        "\n",
        "# Database configuration\n",
        "DB_PATH = \"./enron_emails.db\"\n",
        "EMAIL_DATASET_REPO_ID = \"corbt/enron-emails\"        # í—ˆê¹…í˜ì´ìŠ¤ì— ìˆëŠ” ë°ì´í„°ì…‹ ë ˆí¬ ì•„ì´ë”” \n",
        "SCENARIO_DATASET_REPO_ID = \"corbt/enron_emails_sample_questions\"\n",
        "\n",
        "# Global database connection\n",
        "db_conn = None\n",
        "\n",
        "# í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ì„ ë¡œë“œí•´ì„œ sqlite3ë¡œ dbë¥¼ ë§Œë“¤ê³  sqlite3 dbë¥¼ ì ‘ê·¼í•˜ë©° ì˜³ì€ ëŒ€ë‹µì„ í•´ì£¼ëŠ” LLM Agentë¥¼ ë§Œë“¤ê²ƒì„\n",
        "def create_email_database():\n",
        "    \"\"\"Create the email database from Hugging Face dataset\"\"\"\n",
        "    print(\"Creating email database from Hugging Face dataset...\")\n",
        "    print(\n",
        "        \"This will download and process the full Enron email dataset - this may take several minutes...\"\n",
        "    )\n",
        "\n",
        "    # Database schema\n",
        "    SQL_CREATE_TABLES = \"\"\"\n",
        "    DROP TABLE IF EXISTS recipients;\n",
        "    DROP TABLE IF EXISTS emails_fts;\n",
        "    DROP TABLE IF EXISTS emails;\n",
        "\n",
        "    CREATE TABLE emails (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        message_id TEXT UNIQUE,\n",
        "        subject TEXT,\n",
        "        from_address TEXT,\n",
        "        date TEXT,\n",
        "        body TEXT,\n",
        "        file_name TEXT\n",
        "    );\n",
        "\n",
        "    CREATE TABLE recipients (\n",
        "        email_id TEXT,\n",
        "        recipient_address TEXT,\n",
        "        recipient_type TEXT\n",
        "    );\n",
        "    \"\"\"\n",
        "\n",
        "    SQL_CREATE_INDEXES_TRIGGERS = \"\"\"\n",
        "    CREATE INDEX idx_emails_from ON emails(from_address);\n",
        "    CREATE INDEX idx_emails_date ON emails(date);\n",
        "    CREATE INDEX idx_emails_message_id ON emails(message_id);\n",
        "    CREATE INDEX idx_recipients_address ON recipients(recipient_address);\n",
        "    CREATE INDEX idx_recipients_type ON recipients(recipient_type);\n",
        "    CREATE INDEX idx_recipients_email_id ON recipients(email_id);\n",
        "    CREATE INDEX idx_recipients_address_email ON recipients(recipient_address, email_id);\n",
        "\n",
        "    CREATE VIRTUAL TABLE emails_fts USING fts5(\n",
        "        subject,\n",
        "        body,\n",
        "        content='emails',\n",
        "        content_rowid='id'\n",
        "    );\n",
        "\n",
        "    CREATE TRIGGER emails_ai AFTER INSERT ON emails BEGIN\n",
        "        INSERT INTO emails_fts (rowid, subject, body)\n",
        "        VALUES (new.id, new.subject, new.body);\n",
        "    END;\n",
        "\n",
        "    CREATE TRIGGER emails_ad AFTER DELETE ON emails BEGIN\n",
        "        DELETE FROM emails_fts WHERE rowid=old.id;\n",
        "    END;\n",
        "\n",
        "    CREATE TRIGGER emails_au AFTER UPDATE ON emails BEGIN\n",
        "        UPDATE emails_fts SET subject=new.subject, body=new.body WHERE rowid=old.id;\n",
        "    END;\n",
        "    \"\"\"\n",
        "\n",
        "    # Create database\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.executescript(SQL_CREATE_TABLES)\n",
        "    conn.commit()\n",
        "\n",
        "    # Load dataset\n",
        "    print(\"Loading full email dataset...\")\n",
        "    expected_features = Features(\n",
        "        {\n",
        "            \"message_id\": Value(\"string\"),\n",
        "            \"subject\": Value(\"string\"),\n",
        "            \"from\": Value(\"string\"),\n",
        "            \"to\": Sequence(Value(\"string\")),\n",
        "            \"cc\": Sequence(Value(\"string\")),\n",
        "            \"bcc\": Sequence(Value(\"string\")),\n",
        "            \"date\": Value(\"timestamp[us]\"),\n",
        "            \"body\": Value(\"string\"),\n",
        "            \"file_name\": Value(\"string\"),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    dataset = load_dataset(\n",
        "        EMAIL_DATASET_REPO_ID, features=expected_features, split=\"train\"\n",
        "    )\n",
        "    print(f\"Dataset contains {len(dataset)} total emails\")\n",
        "\n",
        "    # Populate database with ALL emails (not limited to 1000)\n",
        "    print(\"Populating database with all emails...\")\n",
        "    conn.execute(\"PRAGMA synchronous = OFF;\")\n",
        "    conn.execute(\"PRAGMA journal_mode = MEMORY;\")\n",
        "    conn.execute(\"BEGIN TRANSACTION;\")\n",
        "\n",
        "    record_count = 0\n",
        "    skipped_count = 0\n",
        "    duplicate_count = 0\n",
        "    processed_emails = set()  # Track (subject, body, from) tuples for deduplication\n",
        "\n",
        "    for email_data in tqdm(dataset, desc=\"Inserting emails\"):\n",
        "        message_id = email_data[\"message_id\"]\n",
        "        subject = email_data[\"subject\"]\n",
        "        from_address = email_data[\"from\"]\n",
        "        date_obj: datetime = email_data[\"date\"]\n",
        "        body = email_data[\"body\"]\n",
        "        file_name = email_data[\"file_name\"]\n",
        "        to_list = [str(addr) for addr in email_data[\"to\"] if addr]\n",
        "        cc_list = [str(addr) for addr in email_data[\"cc\"] if addr]\n",
        "        bcc_list = [str(addr) for addr in email_data[\"bcc\"] if addr]\n",
        "\n",
        "        # Apply the same filters as the original project\n",
        "        total_recipients = len(to_list) + len(cc_list) + len(bcc_list)\n",
        "\n",
        "        # Filter out very long emails and those with too many recipients\n",
        "        if len(body) > 5000:\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        if total_recipients > 30:\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        # Deduplication check (same as original project)\n",
        "        email_key = (subject, body, from_address)\n",
        "        if email_key in processed_emails:\n",
        "            duplicate_count += 1\n",
        "            continue\n",
        "        else:\n",
        "            processed_emails.add(email_key)\n",
        "\n",
        "        date_str = date_obj.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "        cursor.execute(\n",
        "            \"\"\"\n",
        "            INSERT INTO emails (message_id, subject, from_address, date, body, file_name)\n",
        "            VALUES (?, ?, ?, ?, ?, ?)\n",
        "        \"\"\",\n",
        "            (message_id, subject, from_address, date_str, body, file_name),\n",
        "        )\n",
        "\n",
        "        # Insert recipients\n",
        "        recipient_data = []\n",
        "        for addr in to_list:\n",
        "            recipient_data.append((message_id, addr, \"to\"))\n",
        "        for addr in cc_list:\n",
        "            recipient_data.append((message_id, addr, \"cc\"))\n",
        "        for addr in bcc_list:\n",
        "            recipient_data.append((message_id, addr, \"bcc\"))\n",
        "\n",
        "        if recipient_data:\n",
        "            cursor.executemany(\n",
        "                \"\"\"\n",
        "                INSERT INTO recipients (email_id, recipient_address, recipient_type)\n",
        "                VALUES (?, ?, ?)\n",
        "            \"\"\",\n",
        "                recipient_data,\n",
        "            )\n",
        "\n",
        "        record_count += 1\n",
        "\n",
        "    conn.commit()\n",
        "\n",
        "    # Create indexes and triggers\n",
        "    print(\"Creating indexes and FTS...\")\n",
        "    cursor.executescript(SQL_CREATE_INDEXES_TRIGGERS)\n",
        "    cursor.execute('INSERT INTO emails_fts(emails_fts) VALUES(\"rebuild\")')\n",
        "    conn.commit()\n",
        "\n",
        "    print(f\"Successfully created database with {record_count} emails.\")\n",
        "    print(f\"Skipped {skipped_count} emails due to length/recipient limits.\")\n",
        "    print(f\"Skipped {duplicate_count} duplicate emails.\")\n",
        "    return conn\n",
        "\n",
        "\n",
        "def get_db_connection():\n",
        "    \"\"\"Get database connection\"\"\"\n",
        "    if os.path.exists(DB_PATH):\n",
        "        print(f\"Loading existing database from {DB_PATH}\")\n",
        "        db_conn = sqlite3.connect(DB_PATH, check_same_thread=False)\n",
        "    else:\n",
        "        db_conn = create_email_database()\n",
        "    return db_conn\n",
        "\n",
        "\n",
        "def search_emails(\n",
        "    inbox: str,\n",
        "    keywords: List[str],\n",
        "    from_addr: Optional[str] = None,\n",
        "    to_addr: Optional[str] = None,\n",
        "    sent_after: Optional[str] = None,\n",
        "    sent_before: Optional[str] = None,\n",
        "    max_results: int = 10,\n",
        ") -> List[SearchResult]:\n",
        "    \"\"\"Search the email database based on keywords and filters\"\"\"\n",
        "    conn = get_db_connection()\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    where_clauses: List[str] = []\n",
        "    params: List[str | int] = []\n",
        "\n",
        "    if not keywords:\n",
        "        raise ValueError(\"No keywords provided for search.\")\n",
        "\n",
        "    if max_results > 10:\n",
        "        raise ValueError(\"max_results must be less than or equal to 10.\")\n",
        "\n",
        "    # FTS5 default is AND, so just join keywords. Escape quotes for safety.\n",
        "    fts_query = \" \".join(f\"\"\" \"{k.replace('\"', '\"\"')}\" \"\"\" for k in keywords)\n",
        "    where_clauses.append(\"fts.emails_fts MATCH ?\")\n",
        "    params.append(fts_query)\n",
        "\n",
        "    # Inbox filter\n",
        "    where_clauses.append(\"\"\"\n",
        "        (e.from_address = ? OR EXISTS (\n",
        "            SELECT 1 FROM recipients r_inbox\n",
        "            WHERE r_inbox.recipient_address = ? AND r_inbox.email_id = e.message_id\n",
        "        ))\n",
        "    \"\"\")\n",
        "    params.extend([inbox, inbox])\n",
        "\n",
        "    if from_addr:\n",
        "        where_clauses.append(\"e.from_address = ?\")\n",
        "        params.append(from_addr)\n",
        "\n",
        "    if to_addr:\n",
        "        where_clauses.append(\"\"\"\n",
        "            EXISTS (\n",
        "                SELECT 1 FROM recipients r_to\n",
        "                WHERE r_to.recipient_address = ? AND r_to.email_id = e.message_id\n",
        "            )\n",
        "        \"\"\")\n",
        "        params.append(to_addr)\n",
        "\n",
        "    if sent_after:\n",
        "        where_clauses.append(\"e.date >= ?\")\n",
        "        params.append(f\"{sent_after} 00:00:00\")\n",
        "\n",
        "    if sent_before:\n",
        "        where_clauses.append(\"e.date < ?\")\n",
        "        params.append(f\"{sent_before} 00:00:00\")\n",
        "\n",
        "    sql = f\"\"\"\n",
        "        SELECT\n",
        "            e.message_id,\n",
        "            snippet(emails_fts, -1, '<b>', '</b>', ' ... ', 15) as snippet\n",
        "        FROM\n",
        "            emails e JOIN emails_fts fts ON e.id = fts.rowid\n",
        "        WHERE\n",
        "            {\" AND \".join(where_clauses)}\n",
        "        ORDER BY\n",
        "            e.date DESC\n",
        "        LIMIT ?;\n",
        "    \"\"\"\n",
        "    params.append(max_results)\n",
        "\n",
        "    cursor.execute(sql, params)\n",
        "    results = cursor.fetchall()\n",
        "\n",
        "    return [SearchResult(message_id=row[0], snippet=row[1]) for row in results]\n",
        "\n",
        "\n",
        "def read_email(message_id: str) -> Optional[Email]:\n",
        "    \"\"\"Retrieve a single email by its message_id\"\"\"\n",
        "    conn = get_db_connection()\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Get email details\n",
        "    cursor.execute(\n",
        "        \"SELECT message_id, date, subject, from_address, body, file_name FROM emails WHERE message_id = ?\",\n",
        "        (message_id,),\n",
        "    )\n",
        "    email_row = cursor.fetchone()\n",
        "\n",
        "    if not email_row:\n",
        "        return None\n",
        "\n",
        "    msg_id, date, subject, from_addr, body, file_name = email_row\n",
        "\n",
        "    # Get recipients\n",
        "    cursor.execute(\n",
        "        \"SELECT recipient_address, recipient_type FROM recipients WHERE email_id = ?\",\n",
        "        (message_id,),\n",
        "    )\n",
        "    recipient_rows = cursor.fetchall()\n",
        "\n",
        "    to_addresses = []\n",
        "    cc_addresses = []\n",
        "    bcc_addresses = []\n",
        "\n",
        "    for addr, type_val in recipient_rows:\n",
        "        if type_val.lower() == \"to\":\n",
        "            to_addresses.append(addr)\n",
        "        elif type_val.lower() == \"cc\":\n",
        "            cc_addresses.append(addr)\n",
        "        elif type_val.lower() == \"bcc\":\n",
        "            bcc_addresses.append(addr)\n",
        "\n",
        "    return Email(\n",
        "        message_id=msg_id,\n",
        "        date=date,\n",
        "        subject=subject,\n",
        "        from_address=from_addr,\n",
        "        to_addresses=to_addresses,\n",
        "        cc_addresses=cc_addresses,\n",
        "        bcc_addresses=bcc_addresses,\n",
        "        body=body,\n",
        "        file_name=file_name,\n",
        "    )\n",
        "\n",
        "\n",
        "def load_training_scenarios(\n",
        "    split: Literal[\"train\", \"test\"] = \"train\",\n",
        "    limit: Optional[int] = None,\n",
        "    max_messages: Optional[int] = 1,\n",
        "    shuffle: bool = False,\n",
        "    seed: Optional[int] = None,\n",
        ") -> List[Scenario]:\n",
        "    \"\"\"Load training scenarios from Hugging Face dataset\"\"\"\n",
        "    print(f\"Loading {split} scenarios from Hugging Face...\")\n",
        "    dataset: Dataset = load_dataset(SCENARIO_DATASET_REPO_ID, split=split)\n",
        "\n",
        "    if max_messages is not None:\n",
        "        dataset = dataset.filter(lambda x: len(x[\"message_ids\"]) <= max_messages)\n",
        "\n",
        "    if shuffle or (seed is not None):\n",
        "        if seed is not None:\n",
        "            dataset = dataset.shuffle(seed=seed)\n",
        "        else:\n",
        "            dataset = dataset.shuffle()\n",
        "\n",
        "    # Convert each row to a Scenario object\n",
        "    scenarios = [Scenario(**row, split=split) for row in dataset]\n",
        "\n",
        "    if max_messages is not None:\n",
        "        scenarios = [s for s in scenarios if len(s.message_ids) <= max_messages]\n",
        "\n",
        "    if shuffle:\n",
        "        if seed is not None:\n",
        "            rng = random.Random(seed)\n",
        "            rng.shuffle(scenarios)\n",
        "        else:\n",
        "            random.shuffle(scenarios)\n",
        "\n",
        "    if limit is not None:\n",
        "        scenarios = scenarios[:limit]\n",
        "\n",
        "    print(f\"Loaded {len(scenarios)} scenarios.\")\n",
        "    return scenarios\n",
        "\n",
        "\n",
        "# Load training scenarios\n",
        "training_scenarios = load_training_scenarios(\n",
        "    split=\"train\", limit=50, max_messages=1, shuffle=True, seed=42\n",
        ")\n",
        "\n",
        "print(\"Email search environment created with full Enron dataset!\")\n",
        "print(\n",
        "    f\"Database contains the complete email dataset, loaded {len(training_scenarios)} training scenarios.\"\n",
        ")\n",
        "\n",
        "# print first scenario\n",
        "print(\"\\nSample scenario\")\n",
        "print(\"id:\", training_scenarios[0].id)\n",
        "print(\"question:\", training_scenarios[0].question)\n",
        "print(\"answer:\", training_scenarios[0].answer)\n",
        "print(\"message_ids:\", training_scenarios[0].message_ids)\n",
        "print(\"how_realistic:\", training_scenarios[0].how_realistic)\n",
        "print(\"inbox_address:\", training_scenarios[0].inbox_address)\n",
        "print(\"query_date:\", training_scenarios[0].query_date)\n",
        "print(\"split:\", training_scenarios[0].split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RRi5EtmIX6I"
      },
      "source": [
        "### Creating a Model\n",
        "\n",
        "ì´ì œ í™˜ê²½ ê·œì¹™ì„ ì •ì˜í–ˆìœ¼ë¯€ë¡œ, ì´ë©”ì¼ì„ íš¨ê³¼ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
        "\n",
        "ì´ ì˜ˆì œì—ì„œëŠ” Qwen 2.5 7B ëª¨ë¸ì„ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import art\n",
        "from art.local import LocalBackend\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Declare the model\n",
        "model = art.TrainableModel(\n",
        "    name=\"email-agent-langgraph-001\",\n",
        "    project=\"email-search-agent-langgraph\",\n",
        "    base_model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        ")\n",
        "\n",
        "# To run on a T4, we need to override some config defaults.\n",
        "model._internal_config = art.dev.InternalModelConfig(\n",
        "    init_args=art.dev.InitArgs(\n",
        "        max_seq_length=8192,\n",
        "    ),\n",
        "    engine_args=art.dev.EngineArgs(\n",
        "        enforce_eager=True,\n",
        "        gpu_memory_utilization=0.8,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Initialize the server\n",
        "backend = LocalBackend(\n",
        "    # Normally we don't want to run the server in-process, but for the output\n",
        "    # to show up properly on Google Colab we'll enable this.\n",
        "    in_process=True,\n",
        "    path=\"./.art\",\n",
        ")\n",
        "\n",
        "# Register the model with the local Backend (sets up logging, inference, and training)\n",
        "await model.register(backend)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JImGVAwJIX6J"
      },
      "source": [
        "<a name=\"Rollout\"></a>\n",
        "\n",
        "### Defining a Rollout with LangGraph\n",
        "\n",
        "\n",
        "ë¡¤ì•„ì›ƒì€ ì—ì´ì „íŠ¸ê°€ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë‹¨ì¼ ì—í”¼ì†Œë“œë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì´ ì˜ˆì‹œì—ì„œëŠ” LangGraphì˜ ReAct ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡¤ì•„ì›ƒì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "\n",
        "ë¡¤ì•„ì›ƒ í•¨ìˆ˜ëŠ” ì—ì´ì „íŠ¸ì—ê²Œ **ì´ë©”ì¼ ê²€ìƒ‰** ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì œì‹œí•˜ê³ , LangGraph ì—ì´ì „íŠ¸ëŠ” **ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬**ë¥¼ ì‚¬ìš©í•˜ì—¬ _ì´ë©”ì¼ì„ ê²€ìƒ‰_ í•˜ê³  ì§ˆë¬¸ì— ë‹µí•©ë‹ˆë‹¤.\n",
        "\n",
        "agentê°€ ìµœì¢… ë‹µë³€ì„ ì œê³µí•˜ë©´ ë‹µë³€ì´ ë§ëŠ”ì§€ ì—¬ë¶€ì— ë”°ë¼ `correct` metric ê°€ ê³„ì‚°ë©ë‹ˆë‹¤.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txL32vtjIX6J",
        "outputId": "be12984c-d82b-4088-a246-083eeea55c9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangGraph rollout function defined!\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "\n",
        "import weave\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from litellm import acompletion\n",
        "from tenacity import retry, stop_after_attempt\n",
        "from art.langgraph import init_chat_model\n",
        "\n",
        "import art\n",
        "\n",
        "if os.getenv(\"WANDB_API_KEY\", \"\"):\n",
        "    weave.init(model.project, settings={\"print_call_link\": False})\n",
        "\n",
        "MAX_TURNS = 20\n",
        "\n",
        "class CorrectnessJudgeResponse(BaseModel):\n",
        "    reasoning: str = Field(description=\"Explanation of the reasoning process.\")\n",
        "    accept: bool = Field(description=\"Whether the AI answer should be accepted.\")\n",
        "\n",
        "\n",
        "@retry(stop=stop_after_attempt(3))\n",
        "async def judge_correctness(\n",
        "    scenario: Scenario, answer: str\n",
        ") -> CorrectnessJudgeResponse:\n",
        "    system_prompt = dedent(\n",
        "        \"\"\"\n",
        "        You are given a question, the reference answer (labelled **Reference answer**), and an answer generated by an AI assistant (labelled **AI answer**).\n",
        "\n",
        "        Your task is to decide whether the AI answer is correct and should be accepted. You should accept the answer if it contains the relevant information from the reference answer. You should not accept the answer if it is missing information relevant to the question, or if it contradicts the reference answer.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                f\"Question: {scenario.question}\\n\"\n",
        "                f\"Reference answer: {scenario.answer}\\n\"\n",
        "                f\"AI answer: {answer}\"\n",
        "            ),\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    response = await acompletion(\n",
        "        model=\"openai/gpt-4.1\",\n",
        "        messages=messages,\n",
        "        response_format=CorrectnessJudgeResponse,\n",
        "    )\n",
        "\n",
        "    first_choice = response.choices[0]\n",
        "    raw_content = first_choice.message.content or \"{}\"\n",
        "\n",
        "    try:\n",
        "        return CorrectnessJudgeResponse.model_validate_json(raw_content)\n",
        "    except Exception as e:\n",
        "        return CorrectnessJudgeResponse(\n",
        "            reasoning=f\"Parse error: {e}\\nRaw: {raw_content}\", accept=False\n",
        "        )\n",
        "\n",
        "\n",
        "class ProjectTrajectory(art.Trajectory):\n",
        "    final_answer: FinalAnswer | None = None\n",
        "\n",
        "\n",
        "class EmailScenario(BaseModel):\n",
        "    step: int\n",
        "    scenario: Scenario\n",
        "\n",
        "\n",
        "@weave.op\n",
        "async def rollout(model: art.Model, email_scenario: EmailScenario) -> ProjectTrajectory:\n",
        "    scenario = email_scenario.scenario\n",
        "\n",
        "    traj = ProjectTrajectory(\n",
        "        reward=0.0,\n",
        "        messages_and_choices=[],\n",
        "        metadata={\n",
        "            \"scenario_id\": scenario.id,\n",
        "            \"step\": email_scenario.step,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    system_prompt = dedent(\n",
        "        f\"\"\"\n",
        "        You are an email search agent. You are given a user query and a list of tools you can use to search the user's email. Use the tools to search the user's emails and find the answer to the user's query. You may take up to {MAX_TURNS} turns to find the answer, so if your first search doesn't find the answer, you can try with different keywords.\n",
        "\n",
        "        User's email address is {scenario.inbox_address}\n",
        "        Today's date is {scenario.query_date}\n",
        "\n",
        "        When you have found the answer, use the return_final_answer_tool to provide your final answer along with the source message IDs.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Store final answer in trajectory\n",
        "    final_answer = None\n",
        "\n",
        "    # Define tools inside the rollout function to access local variables\n",
        "    @tool\n",
        "    def search_inbox_tool(keywords: list[str]) -> list[dict]:\n",
        "        \"\"\"Search the inbox for emails matching the given keywords and return\n",
        "        a list of dictionaries so the LLM can easily consume them.\"\"\"\n",
        "        results = search_emails(\n",
        "            inbox=scenario.inbox_address,\n",
        "            keywords=keywords,\n",
        "            sent_before=scenario.query_date,\n",
        "        )\n",
        "        return [asdict(result) for result in results]\n",
        "\n",
        "    @tool\n",
        "    def read_email_tool(message_id: str) -> dict | None:\n",
        "        \"\"\"Read a specific email by message ID.\"\"\"\n",
        "        email = read_email(message_id)\n",
        "        if email:\n",
        "            return email.model_dump()\n",
        "        return None\n",
        "\n",
        "    @tool\n",
        "    def return_final_answer_tool(answer: str, reference_message_ids: list[str]) -> dict:\n",
        "        \"\"\"Return the final answer and the message IDs of the emails that were used to generate the answer.\"\"\"\n",
        "        nonlocal final_answer\n",
        "        final_answer = FinalAnswer(answer=answer, source_ids=reference_message_ids)\n",
        "        return final_answer.model_dump()\n",
        "\n",
        "    # Create LangGraph tools\n",
        "    tools = [search_inbox_tool, read_email_tool, return_final_answer_tool]\n",
        "\n",
        "    chat_model = init_chat_model(model.name, temperature=1.0)\n",
        "\n",
        "    # Create the LangGraph ReAct agent\n",
        "    react_agent = create_react_agent(chat_model, tools)\n",
        "\n",
        "    try:\n",
        "        # Run the agent\n",
        "        config = {\n",
        "            \"configurable\": {\"thread_id\": str(uuid.uuid4())},\n",
        "            \"recursion_limit\": MAX_TURNS,\n",
        "        }\n",
        "\n",
        "        await react_agent.ainvoke(\n",
        "            {\n",
        "                \"messages\": [\n",
        "                    SystemMessage(content=system_prompt),\n",
        "                    HumanMessage(content=scenario.question),\n",
        "                ]\n",
        "            },\n",
        "            config=config,\n",
        "        )\n",
        "\n",
        "        # Check if we got a final answer\n",
        "        if final_answer:\n",
        "            traj.final_answer = final_answer\n",
        "            # Score the trajectory\n",
        "            correctness_judge_response = await judge_correctness(\n",
        "                scenario, traj.final_answer.answer\n",
        "            )\n",
        "            traj.metrics[\"correct\"] = float(correctness_judge_response.accept)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error running LangGraph agent: {e}\")\n",
        "        # Add error information to trajectory\n",
        "        traj.messages_and_choices.append(\n",
        "            {\"role\": \"assistant\", \"content\": f\"Error: {str(e)}\"}\n",
        "        )\n",
        "\n",
        "    return traj\n",
        "\n",
        "\n",
        "print(\"LangGraph rollout function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqPKUCpeIX6J"
      },
      "source": [
        "<a name=\"ruler\"></a>\n",
        "\n",
        "### How RULER works\n",
        "\n",
        "**RULER** ëŠ” ë‹¤ìŒ 2ê°œì˜ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì´ìš©í•©ë‹ˆë‹¤:\n",
        "\n",
        "1. **ìƒëŒ€ì  ì±„ì ì€ ì ˆëŒ€ì  ì±„ì ë³´ë‹¤ ì‰½ìŠµë‹ˆë‹¤**\n",
        "    : LLMì´ ì—¬ëŸ¬ ì†”ë£¨ì…˜ì„ ê°œë³„ì ìœ¼ë¡œ ì±„ì í•˜ëŠ” ê²ƒë³´ë‹¤ ì„œë¡œ ìƒëŒ€ì ìœ¼ë¡œ ìˆœìœ„ë¥¼ ë§¤ê¸°ëŠ” ê²ƒì´ ë” ì‰½ìŠµë‹ˆë‹¤.\n",
        "    \n",
        "2. **GRPOëŠ” ìƒëŒ€ ì ìˆ˜ë§Œ í•„ìš”í•©ë‹ˆë‹¤**\n",
        "    : GRPOëŠ” ê° ê·¸ë£¹ ë‚´ì—ì„œ ì ìˆ˜ë¥¼ ì •ê·œí™”í•˜ë¯€ë¡œ ì ˆëŒ€ ê°’ì´ ì•„ë‹Œ ìƒëŒ€ì  ìˆœìœ„ë§Œ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
        "    \n",
        "\n",
        "#### process:\n",
        "\n",
        "1. ì£¼ì–´ì§„ ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•´ Nê°œì˜ ê¶¤ì ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "2. ëª¨ë“  Nê°œì˜ ê¶¤ì ì„ RULERì— ì „ë‹¬í•©ë‹ˆë‹¤.\n",
        "3. RULERëŠ” ê³µí†µ ì ‘ë‘ì‚¬(ì˜ˆ: ë™ì¼í•œ ì‹œìŠ¤í…œ ë©”ì‹œì§€)ì˜ ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.\n",
        "4. LLM ì‹¬ì‚¬ìœ„ì›ì€ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ì— ë”°ë¼ ê° ê¶¤ì ì„ 0~1ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.\n",
        "5. ì´ ì ìˆ˜ëŠ” GRPO í›ˆë ¨ì—ì„œ ë³´ìƒìœ¼ë¡œ ì§ì ‘ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "To learn more about **RULER**, check out the [RULER docs](https://art.openpipe.ai/fundamentals/ruler).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "0U1aFYfuIX6K",
        "outputId": "e55fcc07-5cd9-456e-d34c-c731f383bd88"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\">[</span>RULER<span style=\"font-weight: bold\">]</span> Pretty-printed LLM choice JSON:\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m[\u001b[0mRULER\u001b[1m]\u001b[0m Pretty-printed LLM choice JSON:\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'scores'</span>: <span style=\"font-weight: bold\">[</span>\n",
              "        <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'trajectory_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'explanation'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Correctly counts from 1 to 10 using numeric symbols as instructed.'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'trajectory_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'explanation'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Shows the correct sequence but uses spelled-out words instead of numeric symbols.'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'trajectory_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'3'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'explanation'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Uses letters rather than numbers; does not fulfill the counting task.'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>\n",
              "        <span style=\"font-weight: bold\">}</span>\n",
              "    <span style=\"font-weight: bold\">]</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'scores'\u001b[0m: \u001b[1m[\u001b[0m\n",
              "        \u001b[1m{\u001b[0m\n",
              "            \u001b[32m'trajectory_id'\u001b[0m: \u001b[32m'1'\u001b[0m,\n",
              "            \u001b[32m'explanation'\u001b[0m: \u001b[32m'Correctly counts from 1 to 10 using numeric symbols as instructed.'\u001b[0m,\n",
              "            \u001b[32m'score'\u001b[0m: \u001b[1;36m1.0\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1m{\u001b[0m\n",
              "            \u001b[32m'trajectory_id'\u001b[0m: \u001b[32m'2'\u001b[0m,\n",
              "            \u001b[32m'explanation'\u001b[0m: \u001b[32m'Shows the correct sequence but uses spelled-out words instead of numeric symbols.'\u001b[0m,\n",
              "            \u001b[32m'score'\u001b[0m: \u001b[1;36m0.5\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1m{\u001b[0m\n",
              "            \u001b[32m'trajectory_id'\u001b[0m: \u001b[32m'3'\u001b[0m,\n",
              "            \u001b[32m'explanation'\u001b[0m: \u001b[32m'Uses letters rather than numbers; does not fulfill the counting task.'\u001b[0m,\n",
              "            \u001b[32m'score'\u001b[0m: \u001b[1;36m0.0\u001b[0m\n",
              "        \u001b[1m}\u001b[0m\n",
              "    \u001b[1m]\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Rank 1: Score 1.000\n",
            "  Response: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10...\n",
            "\n",
            "Rank 2: Score 0.500\n",
            "  Response: one, two, three, four, five, six, seven, eight, ni...\n",
            "\n",
            "Rank 3: Score 0.000\n",
            "  Response: a, b, c, d, e, f, g, h, i, j...\n"
          ]
        }
      ],
      "source": [
        "#@title Sample RULER evaluation\n",
        "\n",
        "import art\n",
        "from art.rewards import ruler_score_group\n",
        "\n",
        "# Test RULER with a simple example\n",
        "base_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You count numbers using numeric symbols.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Count to 10.\"},\n",
        "]\n",
        "\n",
        "good_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\"role\": \"assistant\", \"content\": \"1, 2, 3, 4, 5, 6, 7, 8, 9, 10\"},\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "mediocre_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": \"one, two, three, four, five, six, seven, eight, nine, ten\",\n",
        "        },\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "bad_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\"role\": \"assistant\", \"content\": \"a, b, c, d, e, f, g, h, i, j\"},\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "sample_group = art.TrajectoryGroup(\n",
        "    trajectories=[\n",
        "        good_trajectory,\n",
        "        mediocre_trajectory,\n",
        "        bad_trajectory,\n",
        "    ]\n",
        ")\n",
        "\n",
        "judged_group = await ruler_score_group(sample_group, \"openai/o4-mini\", debug=True)\n",
        "assert judged_group is not None\n",
        "\n",
        "# Display rankings\n",
        "sorted_trajectories = sorted(\n",
        "    judged_group.trajectories, key=lambda t: t.reward, reverse=True\n",
        ")\n",
        "for rank, traj in enumerate(sorted_trajectories, 1):\n",
        "    messages = traj.messages()\n",
        "    print(f\"\\nRank {rank}: Score {traj.reward:.3f}\")\n",
        "    print(f\"  Response: {messages[-1]['content'][:50]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEC3r48iIX6K"
      },
      "source": [
        "<a name=\"Loop\"></a>\n",
        "\n",
        "### Training Loop with LangGraph\n",
        "\n",
        "The training loop is where the magic happens. For each of the steps defined below, the rollout function will be called multiple times in parallel using LangGraph's ReAct agent. Each scenario will produce a trajectory, which will be used to update the model.\n",
        "\n",
        "The `gather` step will wait for all of the trajectories to be generated, then it will use RULER to assign relative scores to each trajectory.\n",
        "\n",
        "Our notebook will then delete all but the most recent checkpoint and train the model on the scored trajectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "from art.utils import iterate_dataset\n",
        "from art.langgraph import wrap_rollout\n",
        "\n",
        "training_config = {\n",
        "    \"groups_per_step\": 2,\n",
        "    \"num_epochs\": 20,\n",
        "    \"rollouts_per_group\": 4,\n",
        "    \"learning_rate\": 1e-5,\n",
        "    \"max_steps\": 20,\n",
        "}\n",
        "\n",
        "# Use iterate_dataset with real training scenarios (similar to train.py)\n",
        "training_iterator = iterate_dataset(\n",
        "    training_scenarios,  # Use real scenarios from Hugging Face\n",
        "    groups_per_step=training_config[\"groups_per_step\"],\n",
        "    num_epochs=training_config[\"num_epochs\"],\n",
        "    initial_step=await model.get_step(),\n",
        ")\n",
        "\n",
        "for batch in training_iterator:\n",
        "    print(\n",
        "        f\"Training step {batch.step}, epoch {batch.epoch}, epoch step {batch.epoch_step}\"\n",
        "    )\n",
        "    print(f\"Batch contains {len(batch.items)} scenarios\")\n",
        "\n",
        "    # Create trajectory groups for this batch (similar to train.py)\n",
        "    groups = []\n",
        "    for scenario in batch.items:\n",
        "        groups.append(\n",
        "            art.TrajectoryGroup(\n",
        "                (\n",
        "                    wrap_rollout(model, rollout)(\n",
        "                        model, EmailScenario(step=batch.step, scenario=scenario)\n",
        "                    )\n",
        "                    for _ in range(training_config[\"rollouts_per_group\"])\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    print(groups[0])\n",
        "    # Gather all trajectory groups\n",
        "    finished_groups = await art.gather_trajectory_groups(\n",
        "        groups,\n",
        "        pbar_desc=\"gather\",\n",
        "        max_exceptions=training_config[\"rollouts_per_group\"] * len(batch.items),\n",
        "    )\n",
        "\n",
        "    judged_groups = []\n",
        "    for group in finished_groups:\n",
        "        # Use RULER to assign relative scores to each trajectory\n",
        "        judged_group = await ruler_score_group(group, \"openai/o4-mini\", debug=True)\n",
        "        judged_groups.append(judged_group)\n",
        "\n",
        "    await model.delete_checkpoints()\n",
        "    await model.train(\n",
        "        judged_groups,\n",
        "        config=art.TrainConfig(learning_rate=training_config[\"learning_rate\"]),\n",
        "        # Lowering the logprob_calculation_chunk_size is a memory saving measure\n",
        "        # to allow longer sequences (up to 8192 tokens) to be processed on a T4.\n",
        "        _config={\"logprob_calculation_chunk_size\": 8},\n",
        "    )\n",
        "\n",
        "    print(f\"Completed training step {batch.step}\")\n",
        "\n",
        "    # Stop after max_steps for demo purposes (adjust as needed)\n",
        "    if batch.step >= training_config[\"max_steps\"]:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wnlNGMlIX6K"
      },
      "source": [
        "### Using the Model\n",
        "\n",
        "LangGraphë¥¼ ì´ìš©í•´ì„œ Agentê°€ emailsë¥¼ ê²€ìƒ‰í•˜ê³  ì§ˆë¬¸ì— ëŒ€ë‹µí•  ìˆ˜ ìˆë„ë¡ í›ˆë ¨ì‹œì¼°ìŠµë‹ˆë‹¤! \n",
        "\n",
        "ì´ì œ training loop ë°–ìœ¼ë¡œ ë‹¹ì‹ ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•  ì‹œê°„ì…ë‹ˆë‹¤. \n",
        "\n",
        "\n",
        "ë°©ê¸ˆ í›ˆë ¨ì‹œí‚¨ ëª¨ë¸ì˜ ë°ëª¨ë¥¼ ìœ„í•´ ì•„ë˜ ì½”ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xvofjjLmIX6L"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing the trained LangGraph model with a real scenario...\n",
            "\n",
            "Test scenario ID: 3751\n",
            "Question: What is my schedule for the week of August 28th?\n",
            "Expected answer: Monday--in office; Tuesday--in office; Wednesday--in office; Thursday--vacation; Friday--vacation.\n",
            "Reference message IDs: ['<28971276.1075842953941.JavaMail.evans@thyme>']\n",
            "Inbox: jeff.dasovich@enron.com\n",
            "Query date: 2000-09-20\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_288833/1452329291.py:133: LangGraphDeprecatedSinceV10: create_react_agent has been moved to langchain.agents. Please update your import to 'from langchain.agents import create_agent'. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  react_agent = create_react_agent(chat_model, tools)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading existing database from ./enron_emails.db\n",
            "Loading existing database from ./enron_emails.db\n",
            "Loading existing database from ./enron_emails.db\n",
            "Loading existing database from ./enron_emails.db\n",
            "Loading existing database from ./enron_emails.db\n",
            "LangGraph Agent's trajectory:\n",
            "--------------------\n",
            "[SYSTEM]: \n",
            "You are an email search agent. You are given a user query and a list of tools you can use to search...\n",
            "\n",
            "[USER]: What is my schedule for the week of August 28th?\n",
            "\n",
            "[ASSISTANT]: [{'id': 'chatcmpl-tool-46d34658dbb743978da665a239ef61fe', 'type': 'function', 'function': {'name': 'search_inbox_tool', 'arguments': '{\"keywords\": [\"schedule\", \"August 28th\"]}'}}]\n",
            "\n",
            "[TOOL - unknown_tool]: \n",
            "\n",
            "[ASSISTANT]: [{'id': 'chatcmpl-tool-d0ad457ef20a45388d5cc2324e7cc073', 'type': 'function', 'function': {'name': 'search_inbox_tool', 'arguments': '{\"keywords\": [\"week\", \"August 28th\", \" agenda\", \"plan\"]}'}}]\n",
            "\n",
            "[TOOL - unknown_tool]: \n",
            "\n",
            "[ASSISTANT]: [{'id': 'chatcmpl-tool-36cfeb1bbe404d56aefc5da06d8dcead', 'type': 'function', 'function': {'name': 'search_inbox_tool', 'arguments': '{\"keywords\": [\"September 4th\", \"follow up\", \" August 28th\"]}'}}]\n",
            "\n",
            "[TOOL - unknown_tool]: \n",
            "\n",
            "[ASSISTANT]: [{'id': 'chatcmpl-tool-7b6d91cd15cb4b579ac5a7fe2d557fac', 'type': 'function', 'function': {'name': 'search_inbox_tool', 'arguments': '{\"keywords\": [\"August 28th\", \"meeting\"]}'}}]\n",
            "\n",
            "[TOOL - unknown_tool]: [{\"message_id\": \"<12993450.1075842953497.JavaMail.evans@thyme>\", \"snippet\": \" ... Conference Call <b>meeting</b> for Monday <b>August 28th</b> at 5:00 PM ED\\n>\\n> Bill Hederman asked ... \"}]\n",
            "\n",
            "[ASSISTANT]: [{'id': 'chatcmpl-tool-866f9c393b1c4d899aa632c9d386f451', 'type': 'function', 'function': {'name': 'read_email_tool', 'arguments': '{\"message_id\": \"<12993450.1075842953497.JavaMail.evans@thyme>\"}'}}]\n",
            "\n",
            "[TOOL - unknown_tool]: {\"message_id\": \"<12993450.1075842953497.JavaMail.evans@thyme>\", \"date\": \"2000-08-26 23:23:00\", \"subject\": \"RE: Conference Call meeting for Monday August 28th at 5:00 PM ED\", \"from_address\": \"amosher@a...\n",
            "\n",
            "[ASSISTANT]: Based on the email found, you had a meeting scheduled for August 28th at 5:00 PM EDT. The details are as follows:\n",
            "\n",
            "- Dial in Number: 1-800-832-3242\n",
            "- Option: 3\n",
            "- Option: 1\n",
            "- Meeting ID: 4321\n",
            "\n",
            "If you need more detailed information or if there were other meetings or activities for the week of August 28th, let me know and we can search for them. \n",
            "\n",
            "The message ID for the reference email is `<12993450.1075842953497.JavaMail.evans@thyme>`.\n",
            "\n",
            "Would you like to search for more information on the schedule for the week of August 28th?\n",
            "\n",
            "--------------------------------------------------\n",
            "No final answer provided by the agent\n",
            "\n",
            "Expected Answer: Monday--in office; Tuesday--in office; Wednesday--in office; Thursday--vacation; Friday--vacation.\n",
            "Expected Source IDs: ['<28971276.1075842953941.JavaMail.evans@thyme>']\n",
            "\n",
            "ğŸ‰ LangGraph email search agent testing completed!\n",
            "The agent used LangGraph's ReAct pattern with the same inference path as during training.\n"
          ]
        }
      ],
      "source": [
        "#@title Loading/inference code\n",
        "\n",
        "# Test the trained model using the rollout function\n",
        "# This avoids memory issues and uses the same inference path as training\n",
        "\n",
        "print(\"Testing the trained LangGraph model with a real scenario...\\n\")\n",
        "\n",
        "\n",
        "# Use a scenario from our training set\n",
        "test_scenario = training_scenarios[1]\n",
        "\n",
        "print(f\"Test scenario ID: {test_scenario.id}\")\n",
        "print(f\"Question: {test_scenario.question}\")\n",
        "print(f\"Expected answer: {test_scenario.answer}\")\n",
        "print(f\"Reference message IDs: {test_scenario.message_ids}\")\n",
        "print(f\"Inbox: {test_scenario.inbox_address}\")\n",
        "print(f\"Query date: {test_scenario.query_date}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Run the rollout function with the trained model\n",
        "test_email_scenario = EmailScenario.model_validate(\n",
        "    {\"step\": 0, \"scenario\": test_scenario.model_dump()}\n",
        ")\n",
        "result_trajectory = await wrap_rollout(model, rollout)(model, test_email_scenario)\n",
        "\n",
        "print(\"LangGraph Agent's trajectory:\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Display the conversation\n",
        "messages = result_trajectory.messages()\n",
        "for i, msg in enumerate(messages):\n",
        "    role = msg.get(\"role\", \"unknown\")\n",
        "    content = msg.get(\"content\", \"\")\n",
        "    tool_calls = msg.get(\"tool_calls\", [])\n",
        "\n",
        "    if role == \"system\":\n",
        "        print(\n",
        "            f\"[SYSTEM]: {content[:100]}...\"\n",
        "            if len(content) > 100\n",
        "            else f\"[SYSTEM]: {content}\"\n",
        "        )\n",
        "    elif role == \"user\":\n",
        "        print(f\"[USER]: {content}\")\n",
        "    elif role == \"assistant\":\n",
        "        if tool_calls:\n",
        "            print(f\"[ASSISTANT]: {tool_calls}\")\n",
        "        if content:\n",
        "            print(f\"[ASSISTANT]: {content}\")\n",
        "    elif role == \"tool\":\n",
        "        tool_name = msg.get(\"name\", \"unknown_tool\")\n",
        "        print(\n",
        "            f\"[TOOL - {tool_name}]: {content[:200]}...\"\n",
        "            if len(content) > 200\n",
        "            else f\"[TOOL - {tool_name}]: {content}\"\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "print(\"-\" * 50)\n",
        "if result_trajectory.final_answer:\n",
        "    print(f\"Agent's Final Answer: {result_trajectory.final_answer.answer}\")\n",
        "    print(f\"Source IDs Used: {result_trajectory.final_answer.source_ids}\")\n",
        "else:\n",
        "    print(\"No final answer provided by the agent\")\n",
        "\n",
        "print(f\"\\nExpected Answer: {test_scenario.answer}\")\n",
        "print(f\"Expected Source IDs: {test_scenario.message_ids}\")\n",
        "\n",
        "print(\"\\nğŸ‰ LangGraph email search agent testing completed!\")\n",
        "print(\n",
        "    \"The agent used LangGraph's ReAct pattern with the same inference path as during training.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omIgMH2KIX6L"
      },
      "source": [
        "<div class=\"align-center\">\n",
        "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Discord.png\" height=\"50\"></a>\n",
        "<a href=\"https://art.openpipe.ai\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Documentation_pill.png\" height=\"50\"></a>\n",
        "\n",
        "Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [Github](https://github.com/openpipe/art).\n",
        "\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "openpipe",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
