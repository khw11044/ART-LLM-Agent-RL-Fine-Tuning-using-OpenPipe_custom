{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emg-y6MMIX6D"
      },
      "source": [
        "## LangGraph를 이용해서 email search agent를 강화학습으로 훈련시키기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Email Search Agent with LangGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이 노트북에서는 LangGraph와 ART를 함께 사용하여 ART•E 에이전트를 처음부터 직접 학습시켜 볼 것입니다! \n",
        "\n",
        "이 구현은 LangGraph의 agent framework와 ART의 training 기능을 통합하는 방법을 보여줍니다.\n",
        "\n",
        "Qwen2.5 - 7B base model로 시작하여, LangGraph의 ReAct agent pattern을 사용하여, 우리는 이것을 훈련 시켜 모델이 이메일을 검색하고 질문에 대답을 하게 합니다.\n",
        "\n",
        "우리는 agentic environment를 구축할 것이고, LangGraph를 이용하여 rollout을 정의하고, training loop를 실행합니다.\n",
        "\n",
        "우리는 또한 agnet의 answers들의 품질을 판단하기 위한 RULER를 어떻게 사용할 것인지 배울 것입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RULER\n",
        "\n",
        "RULER는 agent의 answers 품질을 평가하고, agent가 최상의 완성을 더 잘 생산하도록 훈련시키는 robust한 기술입니다.\n",
        "\n",
        "RULER에 대해서 더 알아보고 싶다면 다음 링크에 접속하세요:\n",
        "[RULER](https://art.openpipe.ai/fundamentals/ruler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QBijDl7iIX6F"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.11.13 environment at: /home/khw/miniconda3/envs/openpipe\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m6 packages\u001b[0m \u001b[2min 82ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#@title 💿 Installation\n",
        "\n",
        "# Portions adapted from Unsloth Notebooks (https://github.com/unslothai/notebooks)\n",
        "# Copyright (c) Unsloth contributors.\n",
        "# License: GNU LGPL v3.0.\n",
        "# Modifications by OpenPipe:\n",
        "# - switched to uv\n",
        "# - changed vllm/triton pinning logic\n",
        "# - added litellm/protobuf pins\n",
        "# See /licenses/LGPL-3.0.txt and /licenses/GPL-3.0.txt for full text.\n",
        "\n",
        "# %%capture\n",
        "import os\n",
        "\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !uv pip install openpipe-art[backend,langgraph]==0.4.11 langchain-core langgraph langchain_openai tenacity datasets --prerelease allow --no-cache-dir\n",
        "else:\n",
        "    try:\n",
        "        import numpy\n",
        "\n",
        "        get_numpy = f\"numpy=={numpy.__version__}\"\n",
        "    except:\n",
        "        get_numpy = \"numpy\"\n",
        "    try:\n",
        "        import subprocess\n",
        "\n",
        "        is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except:\n",
        "        is_t4 = False\n",
        "    get_vllm, get_triton = (\n",
        "        (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm\", \"triton\")\n",
        "    )\n",
        "    !uv pip install --upgrade \\\n",
        "        openpipe-art[backend,langgraph]==0.4.11 langchain-core langgraph langchain_openai tenacity datasets protobuf==5.29.5 {get_vllm} {get_numpy} --prerelease allow --no-cache-dir\n",
        "    !uv pip install -qqq {get_triton}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOMCNkBVIX6G"
      },
      "source": [
        "<a name=\"Environment-Variables\"></a>\n",
        "\n",
        "### Environment Variables\n",
        "\n",
        "**OpenAI (used for RULER judge model)**\n",
        "\n",
        "우리의 RULER 보상 함수는 third-party models을 쿼리하여 에이전트의 performance 품질을 판단합니다. LiteLLM에서 지원하는 모든 모델이 작동합니다. 이 예제에서는 OpenAI의 o4-mini 모델을 사용하므로 `OPENAI_API_KEY` 환경 변수를 설정해야 합니다.\n",
        "\n",
        "\n",
        "\n",
        "**Weights & Biases (optional)**\n",
        "\n",
        "나중에 노트북에서 Weights & Biases에 메트릭을 자동으로 기록하고 Weave에 채팅 완료를 자동으로 기록하는 모델을 만들 것입니다. 이를 위해서는 Weights & Biases API 키를 환경 변수로 제공해야 합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vx9dP2GNIX6G"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Required\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    raise ValueError(\n",
        "        \"OPENAI_API_KEY is required for RULER functionality when using openai/o4-mini.\"\n",
        "    )\n",
        "\n",
        "# Optional\n",
        "# os.environ[\"WANDB_API_KEY\"] = \"YOUR_API_KEY\"\n",
        "\n",
        "if not os.environ.get(\"WANDB_API_KEY\"):\n",
        "    print(\"WANDB_API_KEY is not set. We'll skip logging metrics to Weights & Biases.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9QaTu1CIX6G"
      },
      "source": [
        "<a name=\"Environment\"></a>\n",
        "\n",
        "### Email Search Environment\n",
        "\n",
        "ART는 에이전트가 환경과 상호 작용하여 학습할 수 있도록 합니다. 이 예시에서는 **LangGraph의 도구 통합**을 사용하여 에이전트가 __이메일을 검색하고 관련 질문에 답변할 수 있는 환경__ 을 만들어 보겠습니다.\n",
        "\n",
        "이 agent가 접근 할 수 있는 3가지 도구는 다음과 같습니다:\n",
        "1. `search_inbox` - Search for emails by keywords\n",
        "2. `read_email` - Read a specific email by message ID\n",
        "3. `return_final_answer` - Return the final answer with source email IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9Xvmi-yIX6H",
        "outputId": "d43031fc-4ba8-4d1b-a941-b80462ed519c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/khw/miniconda3/envs/openpipe/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading train scenarios from Hugging Face...\n",
            "Loaded 50 scenarios.\n",
            "Email search environment created with full Enron dataset!\n",
            "Database contains the complete email dataset, loaded 50 training scenarios.\n",
            "\n",
            "Sample scenario\n",
            "id: 3296\n",
            "question: Who can I contact for Power Operations when Sally is in London?\n",
            "answer: Stacey White (x31870) and Leslie Reeves (x37962).\n",
            "message_ids: ['<6033065.1075856098960.JavaMail.evans@thyme>']\n",
            "how_realistic: 0.699999988079071\n",
            "inbox_address: louise.kitchen@enron.com\n",
            "query_date: 2001-01-25\n",
            "split: train\n"
          ]
        }
      ],
      "source": [
        "#@title Email Search Code\n",
        "\n",
        "import os\n",
        "import random\n",
        "import sqlite3\n",
        "from dataclasses import asdict, dataclass\n",
        "from datetime import datetime\n",
        "from textwrap import dedent\n",
        "from typing import List, Literal, Optional\n",
        "\n",
        "from datasets import Dataset, Features, Sequence, Value, load_dataset\n",
        "from pydantic import BaseModel, Field\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Email and Scenario data models\n",
        "class Email(BaseModel):\n",
        "    message_id: str\n",
        "    date: str  # ISO 8601 string 'YYYY-MM-DD HH:MM:SS'\n",
        "    subject: Optional[str] = None\n",
        "    from_address: Optional[str] = None\n",
        "    to_addresses: List[str] = []  # Populated from recipients table\n",
        "    cc_addresses: List[str] = []  # Populated from recipients table\n",
        "    bcc_addresses: List[str] = []  # Populated from recipients table\n",
        "    body: Optional[str] = None\n",
        "    file_name: Optional[str] = None\n",
        "\n",
        "\n",
        "class Scenario(BaseModel):\n",
        "    id: int\n",
        "    question: str\n",
        "    answer: str\n",
        "    message_ids: List[str]  # message_ids (strings) of referenced emails\n",
        "    how_realistic: float\n",
        "    inbox_address: str\n",
        "    query_date: str\n",
        "    split: Literal[\"train\", \"test\"]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SearchResult:\n",
        "    message_id: str\n",
        "    snippet: str\n",
        "\n",
        "\n",
        "class FinalAnswer(BaseModel):\n",
        "    answer: str\n",
        "    source_ids: list[str]\n",
        "\n",
        "\n",
        "# Database configuration\n",
        "DB_PATH = \"./enron_emails.db\"\n",
        "EMAIL_DATASET_REPO_ID = \"corbt/enron-emails\"        # 허깅페이스에 있는 데이터셋 레포 아이디 \n",
        "SCENARIO_DATASET_REPO_ID = \"corbt/enron_emails_sample_questions\"\n",
        "\n",
        "# Global database connection\n",
        "db_conn = None\n",
        "\n",
        "# 허깅페이스 데이터셋을 로드해서 sqlite3로 db를 만들고 sqlite3 db를 접근하며 옳은 대답을 해주는 LLM Agent를 만들것임\n",
        "def create_email_database():\n",
        "    \"\"\"Create the email database from Hugging Face dataset\"\"\"\n",
        "    print(\"Creating email database from Hugging Face dataset...\")\n",
        "    print(\n",
        "        \"This will download and process the full Enron email dataset - this may take several minutes...\"\n",
        "    )\n",
        "\n",
        "    # Database schema\n",
        "    SQL_CREATE_TABLES = \"\"\"\n",
        "    DROP TABLE IF EXISTS recipients;\n",
        "    DROP TABLE IF EXISTS emails_fts;\n",
        "    DROP TABLE IF EXISTS emails;\n",
        "\n",
        "    CREATE TABLE emails (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        message_id TEXT UNIQUE,\n",
        "        subject TEXT,\n",
        "        from_address TEXT,\n",
        "        date TEXT,\n",
        "        body TEXT,\n",
        "        file_name TEXT\n",
        "    );\n",
        "\n",
        "    CREATE TABLE recipients (\n",
        "        email_id TEXT,\n",
        "        recipient_address TEXT,\n",
        "        recipient_type TEXT\n",
        "    );\n",
        "    \"\"\"\n",
        "\n",
        "    SQL_CREATE_INDEXES_TRIGGERS = \"\"\"\n",
        "    CREATE INDEX idx_emails_from ON emails(from_address);\n",
        "    CREATE INDEX idx_emails_date ON emails(date);\n",
        "    CREATE INDEX idx_emails_message_id ON emails(message_id);\n",
        "    CREATE INDEX idx_recipients_address ON recipients(recipient_address);\n",
        "    CREATE INDEX idx_recipients_type ON recipients(recipient_type);\n",
        "    CREATE INDEX idx_recipients_email_id ON recipients(email_id);\n",
        "    CREATE INDEX idx_recipients_address_email ON recipients(recipient_address, email_id);\n",
        "\n",
        "    CREATE VIRTUAL TABLE emails_fts USING fts5(\n",
        "        subject,\n",
        "        body,\n",
        "        content='emails',\n",
        "        content_rowid='id'\n",
        "    );\n",
        "\n",
        "    CREATE TRIGGER emails_ai AFTER INSERT ON emails BEGIN\n",
        "        INSERT INTO emails_fts (rowid, subject, body)\n",
        "        VALUES (new.id, new.subject, new.body);\n",
        "    END;\n",
        "\n",
        "    CREATE TRIGGER emails_ad AFTER DELETE ON emails BEGIN\n",
        "        DELETE FROM emails_fts WHERE rowid=old.id;\n",
        "    END;\n",
        "\n",
        "    CREATE TRIGGER emails_au AFTER UPDATE ON emails BEGIN\n",
        "        UPDATE emails_fts SET subject=new.subject, body=new.body WHERE rowid=old.id;\n",
        "    END;\n",
        "    \"\"\"\n",
        "\n",
        "    # Create database\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.executescript(SQL_CREATE_TABLES)\n",
        "    conn.commit()\n",
        "\n",
        "    # Load dataset\n",
        "    print(\"Loading full email dataset...\")\n",
        "    expected_features = Features(\n",
        "        {\n",
        "            \"message_id\": Value(\"string\"),\n",
        "            \"subject\": Value(\"string\"),\n",
        "            \"from\": Value(\"string\"),\n",
        "            \"to\": Sequence(Value(\"string\")),\n",
        "            \"cc\": Sequence(Value(\"string\")),\n",
        "            \"bcc\": Sequence(Value(\"string\")),\n",
        "            \"date\": Value(\"timestamp[us]\"),\n",
        "            \"body\": Value(\"string\"),\n",
        "            \"file_name\": Value(\"string\"),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    dataset = load_dataset(\n",
        "        EMAIL_DATASET_REPO_ID, features=expected_features, split=\"train\"\n",
        "    )\n",
        "    print(f\"Dataset contains {len(dataset)} total emails\")\n",
        "\n",
        "    # Populate database with ALL emails (not limited to 1000)\n",
        "    print(\"Populating database with all emails...\")\n",
        "    conn.execute(\"PRAGMA synchronous = OFF;\")\n",
        "    conn.execute(\"PRAGMA journal_mode = MEMORY;\")\n",
        "    conn.execute(\"BEGIN TRANSACTION;\")\n",
        "\n",
        "    record_count = 0\n",
        "    skipped_count = 0\n",
        "    duplicate_count = 0\n",
        "    processed_emails = set()  # Track (subject, body, from) tuples for deduplication\n",
        "\n",
        "    for email_data in tqdm(dataset, desc=\"Inserting emails\"):\n",
        "        message_id = email_data[\"message_id\"]\n",
        "        subject = email_data[\"subject\"]\n",
        "        from_address = email_data[\"from\"]\n",
        "        date_obj: datetime = email_data[\"date\"]\n",
        "        body = email_data[\"body\"]\n",
        "        file_name = email_data[\"file_name\"]\n",
        "        to_list = [str(addr) for addr in email_data[\"to\"] if addr]\n",
        "        cc_list = [str(addr) for addr in email_data[\"cc\"] if addr]\n",
        "        bcc_list = [str(addr) for addr in email_data[\"bcc\"] if addr]\n",
        "\n",
        "        # Apply the same filters as the original project\n",
        "        total_recipients = len(to_list) + len(cc_list) + len(bcc_list)\n",
        "\n",
        "        # Filter out very long emails and those with too many recipients\n",
        "        if len(body) > 5000:\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        if total_recipients > 30:\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        # Deduplication check (same as original project)\n",
        "        email_key = (subject, body, from_address)\n",
        "        if email_key in processed_emails:\n",
        "            duplicate_count += 1\n",
        "            continue\n",
        "        else:\n",
        "            processed_emails.add(email_key)\n",
        "\n",
        "        date_str = date_obj.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "        cursor.execute(\n",
        "            \"\"\"\n",
        "            INSERT INTO emails (message_id, subject, from_address, date, body, file_name)\n",
        "            VALUES (?, ?, ?, ?, ?, ?)\n",
        "        \"\"\",\n",
        "            (message_id, subject, from_address, date_str, body, file_name),\n",
        "        )\n",
        "\n",
        "        # Insert recipients\n",
        "        recipient_data = []\n",
        "        for addr in to_list:\n",
        "            recipient_data.append((message_id, addr, \"to\"))\n",
        "        for addr in cc_list:\n",
        "            recipient_data.append((message_id, addr, \"cc\"))\n",
        "        for addr in bcc_list:\n",
        "            recipient_data.append((message_id, addr, \"bcc\"))\n",
        "\n",
        "        if recipient_data:\n",
        "            cursor.executemany(\n",
        "                \"\"\"\n",
        "                INSERT INTO recipients (email_id, recipient_address, recipient_type)\n",
        "                VALUES (?, ?, ?)\n",
        "            \"\"\",\n",
        "                recipient_data,\n",
        "            )\n",
        "\n",
        "        record_count += 1\n",
        "\n",
        "    conn.commit()\n",
        "\n",
        "    # Create indexes and triggers\n",
        "    print(\"Creating indexes and FTS...\")\n",
        "    cursor.executescript(SQL_CREATE_INDEXES_TRIGGERS)\n",
        "    cursor.execute('INSERT INTO emails_fts(emails_fts) VALUES(\"rebuild\")')\n",
        "    conn.commit()\n",
        "\n",
        "    print(f\"Successfully created database with {record_count} emails.\")\n",
        "    print(f\"Skipped {skipped_count} emails due to length/recipient limits.\")\n",
        "    print(f\"Skipped {duplicate_count} duplicate emails.\")\n",
        "    return conn\n",
        "\n",
        "\n",
        "def get_db_connection():\n",
        "    \"\"\"Get database connection\"\"\"\n",
        "    if os.path.exists(DB_PATH):\n",
        "        print(f\"Loading existing database from {DB_PATH}\")\n",
        "        db_conn = sqlite3.connect(DB_PATH, check_same_thread=False)\n",
        "    else:\n",
        "        db_conn = create_email_database()\n",
        "    return db_conn\n",
        "\n",
        "\n",
        "def search_emails(\n",
        "    inbox: str,\n",
        "    keywords: List[str],\n",
        "    from_addr: Optional[str] = None,\n",
        "    to_addr: Optional[str] = None,\n",
        "    sent_after: Optional[str] = None,\n",
        "    sent_before: Optional[str] = None,\n",
        "    max_results: int = 10,\n",
        ") -> List[SearchResult]:\n",
        "    \"\"\"Search the email database based on keywords and filters\"\"\"\n",
        "    conn = get_db_connection()\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    where_clauses: List[str] = []\n",
        "    params: List[str | int] = []\n",
        "\n",
        "    if not keywords:\n",
        "        raise ValueError(\"No keywords provided for search.\")\n",
        "\n",
        "    if max_results > 10:\n",
        "        raise ValueError(\"max_results must be less than or equal to 10.\")\n",
        "\n",
        "    # FTS5 default is AND, so just join keywords. Escape quotes for safety.\n",
        "    fts_query = \" \".join(f\"\"\" \"{k.replace('\"', '\"\"')}\" \"\"\" for k in keywords)\n",
        "    where_clauses.append(\"fts.emails_fts MATCH ?\")\n",
        "    params.append(fts_query)\n",
        "\n",
        "    # Inbox filter\n",
        "    where_clauses.append(\"\"\"\n",
        "        (e.from_address = ? OR EXISTS (\n",
        "            SELECT 1 FROM recipients r_inbox\n",
        "            WHERE r_inbox.recipient_address = ? AND r_inbox.email_id = e.message_id\n",
        "        ))\n",
        "    \"\"\")\n",
        "    params.extend([inbox, inbox])\n",
        "\n",
        "    if from_addr:\n",
        "        where_clauses.append(\"e.from_address = ?\")\n",
        "        params.append(from_addr)\n",
        "\n",
        "    if to_addr:\n",
        "        where_clauses.append(\"\"\"\n",
        "            EXISTS (\n",
        "                SELECT 1 FROM recipients r_to\n",
        "                WHERE r_to.recipient_address = ? AND r_to.email_id = e.message_id\n",
        "            )\n",
        "        \"\"\")\n",
        "        params.append(to_addr)\n",
        "\n",
        "    if sent_after:\n",
        "        where_clauses.append(\"e.date >= ?\")\n",
        "        params.append(f\"{sent_after} 00:00:00\")\n",
        "\n",
        "    if sent_before:\n",
        "        where_clauses.append(\"e.date < ?\")\n",
        "        params.append(f\"{sent_before} 00:00:00\")\n",
        "\n",
        "    sql = f\"\"\"\n",
        "        SELECT\n",
        "            e.message_id,\n",
        "            snippet(emails_fts, -1, '<b>', '</b>', ' ... ', 15) as snippet\n",
        "        FROM\n",
        "            emails e JOIN emails_fts fts ON e.id = fts.rowid\n",
        "        WHERE\n",
        "            {\" AND \".join(where_clauses)}\n",
        "        ORDER BY\n",
        "            e.date DESC\n",
        "        LIMIT ?;\n",
        "    \"\"\"\n",
        "    params.append(max_results)\n",
        "\n",
        "    cursor.execute(sql, params)\n",
        "    results = cursor.fetchall()\n",
        "\n",
        "    return [SearchResult(message_id=row[0], snippet=row[1]) for row in results]\n",
        "\n",
        "\n",
        "def read_email(message_id: str) -> Optional[Email]:\n",
        "    \"\"\"Retrieve a single email by its message_id\"\"\"\n",
        "    conn = get_db_connection()\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Get email details\n",
        "    cursor.execute(\n",
        "        \"SELECT message_id, date, subject, from_address, body, file_name FROM emails WHERE message_id = ?\",\n",
        "        (message_id,),\n",
        "    )\n",
        "    email_row = cursor.fetchone()\n",
        "\n",
        "    if not email_row:\n",
        "        return None\n",
        "\n",
        "    msg_id, date, subject, from_addr, body, file_name = email_row\n",
        "\n",
        "    # Get recipients\n",
        "    cursor.execute(\n",
        "        \"SELECT recipient_address, recipient_type FROM recipients WHERE email_id = ?\",\n",
        "        (message_id,),\n",
        "    )\n",
        "    recipient_rows = cursor.fetchall()\n",
        "\n",
        "    to_addresses = []\n",
        "    cc_addresses = []\n",
        "    bcc_addresses = []\n",
        "\n",
        "    for addr, type_val in recipient_rows:\n",
        "        if type_val.lower() == \"to\":\n",
        "            to_addresses.append(addr)\n",
        "        elif type_val.lower() == \"cc\":\n",
        "            cc_addresses.append(addr)\n",
        "        elif type_val.lower() == \"bcc\":\n",
        "            bcc_addresses.append(addr)\n",
        "\n",
        "    return Email(\n",
        "        message_id=msg_id,\n",
        "        date=date,\n",
        "        subject=subject,\n",
        "        from_address=from_addr,\n",
        "        to_addresses=to_addresses,\n",
        "        cc_addresses=cc_addresses,\n",
        "        bcc_addresses=bcc_addresses,\n",
        "        body=body,\n",
        "        file_name=file_name,\n",
        "    )\n",
        "\n",
        "\n",
        "def load_training_scenarios(\n",
        "    split: Literal[\"train\", \"test\"] = \"train\",\n",
        "    limit: Optional[int] = None,\n",
        "    max_messages: Optional[int] = 1,\n",
        "    shuffle: bool = False,\n",
        "    seed: Optional[int] = None,\n",
        ") -> List[Scenario]:\n",
        "    \"\"\"Load training scenarios from Hugging Face dataset\"\"\"\n",
        "    print(f\"Loading {split} scenarios from Hugging Face...\")\n",
        "    dataset: Dataset = load_dataset(SCENARIO_DATASET_REPO_ID, split=split)\n",
        "\n",
        "    if max_messages is not None:\n",
        "        dataset = dataset.filter(lambda x: len(x[\"message_ids\"]) <= max_messages)\n",
        "\n",
        "    if shuffle or (seed is not None):\n",
        "        if seed is not None:\n",
        "            dataset = dataset.shuffle(seed=seed)\n",
        "        else:\n",
        "            dataset = dataset.shuffle()\n",
        "\n",
        "    # Convert each row to a Scenario object\n",
        "    scenarios = [Scenario(**row, split=split) for row in dataset]\n",
        "\n",
        "    if max_messages is not None:\n",
        "        scenarios = [s for s in scenarios if len(s.message_ids) <= max_messages]\n",
        "\n",
        "    if shuffle:\n",
        "        if seed is not None:\n",
        "            rng = random.Random(seed)\n",
        "            rng.shuffle(scenarios)\n",
        "        else:\n",
        "            random.shuffle(scenarios)\n",
        "\n",
        "    if limit is not None:\n",
        "        scenarios = scenarios[:limit]\n",
        "\n",
        "    print(f\"Loaded {len(scenarios)} scenarios.\")\n",
        "    return scenarios\n",
        "\n",
        "\n",
        "# Load training scenarios\n",
        "training_scenarios = load_training_scenarios(\n",
        "    split=\"train\", limit=50, max_messages=1, shuffle=True, seed=42\n",
        ")\n",
        "\n",
        "print(\"Email search environment created with full Enron dataset!\")\n",
        "print(\n",
        "    f\"Database contains the complete email dataset, loaded {len(training_scenarios)} training scenarios.\"\n",
        ")\n",
        "\n",
        "# print first scenario\n",
        "print(\"\\nSample scenario\")\n",
        "print(\"id:\", training_scenarios[0].id)\n",
        "print(\"question:\", training_scenarios[0].question)\n",
        "print(\"answer:\", training_scenarios[0].answer)\n",
        "print(\"message_ids:\", training_scenarios[0].message_ids)\n",
        "print(\"how_realistic:\", training_scenarios[0].how_realistic)\n",
        "print(\"inbox_address:\", training_scenarios[0].inbox_address)\n",
        "print(\"query_date:\", training_scenarios[0].query_date)\n",
        "print(\"split:\", training_scenarios[0].split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RRi5EtmIX6I"
      },
      "source": [
        "### Creating a Model\n",
        "\n",
        "이제 환경 규칙을 정의했으므로, 이메일을 효과적으로 검색하는 방법을 학습하는 모델을 만들 수 있습니다. \n",
        "\n",
        "이 예제에서는 Qwen 2.5 7B 모델을 사용하겠습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import art\n",
        "from art.local import LocalBackend\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Declare the model\n",
        "model = art.TrainableModel(\n",
        "    name=\"email-agent-langgraph-001\",\n",
        "    project=\"email-search-agent-langgraph\",\n",
        "    base_model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        ")\n",
        "\n",
        "# To run on a T4, we need to override some config defaults.\n",
        "model._internal_config = art.dev.InternalModelConfig(\n",
        "    init_args=art.dev.InitArgs(\n",
        "        max_seq_length=8192,\n",
        "    ),\n",
        "    engine_args=art.dev.EngineArgs(\n",
        "        enforce_eager=True,\n",
        "        gpu_memory_utilization=0.8,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Initialize the server\n",
        "backend = LocalBackend(\n",
        "    # Normally we don't want to run the server in-process, but for the output\n",
        "    # to show up properly on Google Colab we'll enable this.\n",
        "    in_process=True,\n",
        "    path=\"./.art\",\n",
        ")\n",
        "\n",
        "# Register the model with the local Backend (sets up logging, inference, and training)\n",
        "await model.register(backend)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JImGVAwJIX6J"
      },
      "source": [
        "<a name=\"Rollout\"></a>\n",
        "\n",
        "### Defining a Rollout with LangGraph\n",
        "\n",
        "\n",
        "롤아웃은 에이전트가 작업을 수행하는 단일 에피소드를 의미합니다.\n",
        "\n",
        "이 예시에서는 LangGraph의 ReAct 에이전트를 사용하여 롤아웃을 처리합니다.\n",
        "\n",
        "롤아웃 함수는 에이전트에게 **이메일 검색** 시나리오를 제시하고, LangGraph 에이전트는 **사용 가능한 도구**를 사용하여 _이메일을 검색_ 하고 질문에 답합니다.\n",
        "\n",
        "agent가 최종 답변을 제공하면 답변이 맞는지 여부에 따라 `correct` metric 가 계산됩니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txL32vtjIX6J",
        "outputId": "be12984c-d82b-4088-a246-083eeea55c9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangGraph rollout function defined!\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "\n",
        "import weave\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from litellm import acompletion\n",
        "from tenacity import retry, stop_after_attempt\n",
        "from art.langgraph import init_chat_model\n",
        "\n",
        "import art\n",
        "\n",
        "if os.getenv(\"WANDB_API_KEY\", \"\"):\n",
        "    weave.init(model.project, settings={\"print_call_link\": False})\n",
        "\n",
        "MAX_TURNS = 20\n",
        "\n",
        "class CorrectnessJudgeResponse(BaseModel):\n",
        "    reasoning: str = Field(description=\"Explanation of the reasoning process.\")\n",
        "    accept: bool = Field(description=\"Whether the AI answer should be accepted.\")\n",
        "\n",
        "\n",
        "@retry(stop=stop_after_attempt(3))\n",
        "async def judge_correctness(\n",
        "    scenario: Scenario, answer: str\n",
        ") -> CorrectnessJudgeResponse:\n",
        "    system_prompt = dedent(\n",
        "        \"\"\"\n",
        "        You are given a question, the reference answer (labelled **Reference answer**), and an answer generated by an AI assistant (labelled **AI answer**).\n",
        "\n",
        "        Your task is to decide whether the AI answer is correct and should be accepted. You should accept the answer if it contains the relevant information from the reference answer. You should not accept the answer if it is missing information relevant to the question, or if it contradicts the reference answer.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                f\"Question: {scenario.question}\\n\"\n",
        "                f\"Reference answer: {scenario.answer}\\n\"\n",
        "                f\"AI answer: {answer}\"\n",
        "            ),\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    response = await acompletion(\n",
        "        model=\"openai/gpt-4.1\",\n",
        "        messages=messages,\n",
        "        response_format=CorrectnessJudgeResponse,\n",
        "    )\n",
        "\n",
        "    first_choice = response.choices[0]\n",
        "    raw_content = first_choice.message.content or \"{}\"\n",
        "\n",
        "    try:\n",
        "        return CorrectnessJudgeResponse.model_validate_json(raw_content)\n",
        "    except Exception as e:\n",
        "        return CorrectnessJudgeResponse(\n",
        "            reasoning=f\"Parse error: {e}\\nRaw: {raw_content}\", accept=False\n",
        "        )\n",
        "\n",
        "\n",
        "class ProjectTrajectory(art.Trajectory):\n",
        "    final_answer: FinalAnswer | None = None\n",
        "\n",
        "\n",
        "class EmailScenario(BaseModel):\n",
        "    step: int\n",
        "    scenario: Scenario\n",
        "\n",
        "\n",
        "@weave.op\n",
        "async def rollout(model: art.Model, email_scenario: EmailScenario) -> ProjectTrajectory:\n",
        "    scenario = email_scenario.scenario\n",
        "\n",
        "    traj = ProjectTrajectory(\n",
        "        reward=0.0,\n",
        "        messages_and_choices=[],\n",
        "        metadata={\n",
        "            \"scenario_id\": scenario.id,\n",
        "            \"step\": email_scenario.step,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    system_prompt = dedent(\n",
        "        f\"\"\"\n",
        "        You are an email search agent. You are given a user query and a list of tools you can use to search the user's email. Use the tools to search the user's emails and find the answer to the user's query. You may take up to {MAX_TURNS} turns to find the answer, so if your first search doesn't find the answer, you can try with different keywords.\n",
        "\n",
        "        User's email address is {scenario.inbox_address}\n",
        "        Today's date is {scenario.query_date}\n",
        "\n",
        "        When you have found the answer, use the return_final_answer_tool to provide your final answer along with the source message IDs.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Store final answer in trajectory\n",
        "    final_answer = None\n",
        "\n",
        "    # Define tools inside the rollout function to access local variables\n",
        "    @tool\n",
        "    def search_inbox_tool(keywords: list[str]) -> list[dict]:\n",
        "        \"\"\"Search the inbox for emails matching the given keywords and return\n",
        "        a list of dictionaries so the LLM can easily consume them.\"\"\"\n",
        "        results = search_emails(\n",
        "            inbox=scenario.inbox_address,\n",
        "            keywords=keywords,\n",
        "            sent_before=scenario.query_date,\n",
        "        )\n",
        "        return [asdict(result) for result in results]\n",
        "\n",
        "    @tool\n",
        "    def read_email_tool(message_id: str) -> dict | None:\n",
        "        \"\"\"Read a specific email by message ID.\"\"\"\n",
        "        email = read_email(message_id)\n",
        "        if email:\n",
        "            return email.model_dump()\n",
        "        return None\n",
        "\n",
        "    @tool\n",
        "    def return_final_answer_tool(answer: str, reference_message_ids: list[str]) -> dict:\n",
        "        \"\"\"Return the final answer and the message IDs of the emails that were used to generate the answer.\"\"\"\n",
        "        nonlocal final_answer\n",
        "        final_answer = FinalAnswer(answer=answer, source_ids=reference_message_ids)\n",
        "        return final_answer.model_dump()\n",
        "\n",
        "    # Create LangGraph tools\n",
        "    tools = [search_inbox_tool, read_email_tool, return_final_answer_tool]\n",
        "\n",
        "    chat_model = init_chat_model(model.name, temperature=1.0)\n",
        "\n",
        "    # Create the LangGraph ReAct agent\n",
        "    react_agent = create_react_agent(chat_model, tools)\n",
        "\n",
        "    try:\n",
        "        # Run the agent\n",
        "        config = {\n",
        "            \"configurable\": {\"thread_id\": str(uuid.uuid4())},\n",
        "            \"recursion_limit\": MAX_TURNS,\n",
        "        }\n",
        "\n",
        "        await react_agent.ainvoke(\n",
        "            {\n",
        "                \"messages\": [\n",
        "                    SystemMessage(content=system_prompt),\n",
        "                    HumanMessage(content=scenario.question),\n",
        "                ]\n",
        "            },\n",
        "            config=config,\n",
        "        )\n",
        "\n",
        "        # Check if we got a final answer\n",
        "        if final_answer:\n",
        "            traj.final_answer = final_answer\n",
        "            # Score the trajectory\n",
        "            correctness_judge_response = await judge_correctness(\n",
        "                scenario, traj.final_answer.answer\n",
        "            )\n",
        "            traj.metrics[\"correct\"] = float(correctness_judge_response.accept)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error running LangGraph agent: {e}\")\n",
        "        # Add error information to trajectory\n",
        "        traj.messages_and_choices.append(\n",
        "            {\"role\": \"assistant\", \"content\": f\"Error: {str(e)}\"}\n",
        "        )\n",
        "\n",
        "    return traj\n",
        "\n",
        "\n",
        "print(\"LangGraph rollout function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqPKUCpeIX6J"
      },
      "source": [
        "<a name=\"ruler\"></a>\n",
        "\n",
        "### How RULER works\n",
        "\n",
        "**RULER** 는 다음 2개의 핵심 인사이트를 이용합니다:\n",
        "\n",
        "1. **상대적 채점은 절대적 채점보다 쉽습니다**\n",
        "    : LLM이 여러 솔루션을 개별적으로 채점하는 것보다 서로 상대적으로 순위를 매기는 것이 더 쉽습니다.\n",
        "    \n",
        "2. **GRPO는 상대 점수만 필요합니다**\n",
        "    : GRPO는 각 그룹 내에서 점수를 정규화하므로 절대 값이 아닌 상대적 순위만 중요합니다.\n",
        "    \n",
        "\n",
        "#### process:\n",
        "\n",
        "1. 주어진 시나리오에 대해 N개의 궤적을 생성합니다.\n",
        "2. 모든 N개의 궤적을 RULER에 전달합니다.\n",
        "3. RULER는 공통 접두사(예: 동일한 시스템 메시지)의 중복을 제거합니다.\n",
        "4. LLM 심사위원은 목표 달성 여부에 따라 각 궤적을 0~1점으로 평가합니다.\n",
        "5. 이 점수는 GRPO 훈련에서 보상으로 직접 사용됩니다.\n",
        "\n",
        "\n",
        "To learn more about **RULER**, check out the [RULER docs](https://art.openpipe.ai/fundamentals/ruler).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "0U1aFYfuIX6K",
        "outputId": "e55fcc07-5cd9-456e-d34c-c731f383bd88"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\">[</span>RULER<span style=\"font-weight: bold\">]</span> Pretty-printed LLM choice JSON:\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m[\u001b[0mRULER\u001b[1m]\u001b[0m Pretty-printed LLM choice JSON:\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'scores'</span>: <span style=\"font-weight: bold\">[</span>\n",
              "        <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'trajectory_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'explanation'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Correctly counts from 1 to 10 using numeric symbols as instructed.'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'trajectory_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'explanation'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Shows the correct sequence but uses spelled-out words instead of numeric symbols.'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'trajectory_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'3'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'explanation'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Uses letters rather than numbers; does not fulfill the counting task.'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>\n",
              "        <span style=\"font-weight: bold\">}</span>\n",
              "    <span style=\"font-weight: bold\">]</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'scores'\u001b[0m: \u001b[1m[\u001b[0m\n",
              "        \u001b[1m{\u001b[0m\n",
              "            \u001b[32m'trajectory_id'\u001b[0m: \u001b[32m'1'\u001b[0m,\n",
              "            \u001b[32m'explanation'\u001b[0m: \u001b[32m'Correctly counts from 1 to 10 using numeric symbols as instructed.'\u001b[0m,\n",
              "            \u001b[32m'score'\u001b[0m: \u001b[1;36m1.0\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1m{\u001b[0m\n",
              "            \u001b[32m'trajectory_id'\u001b[0m: \u001b[32m'2'\u001b[0m,\n",
              "            \u001b[32m'explanation'\u001b[0m: \u001b[32m'Shows the correct sequence but uses spelled-out words instead of numeric symbols.'\u001b[0m,\n",
              "            \u001b[32m'score'\u001b[0m: \u001b[1;36m0.5\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1m{\u001b[0m\n",
              "            \u001b[32m'trajectory_id'\u001b[0m: \u001b[32m'3'\u001b[0m,\n",
              "            \u001b[32m'explanation'\u001b[0m: \u001b[32m'Uses letters rather than numbers; does not fulfill the counting task.'\u001b[0m,\n",
              "            \u001b[32m'score'\u001b[0m: \u001b[1;36m0.0\u001b[0m\n",
              "        \u001b[1m}\u001b[0m\n",
              "    \u001b[1m]\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Rank 1: Score 1.000\n",
            "  Response: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10...\n",
            "\n",
            "Rank 2: Score 0.500\n",
            "  Response: one, two, three, four, five, six, seven, eight, ni...\n",
            "\n",
            "Rank 3: Score 0.000\n",
            "  Response: a, b, c, d, e, f, g, h, i, j...\n"
          ]
        }
      ],
      "source": [
        "#@title Sample RULER evaluation\n",
        "\n",
        "import art\n",
        "from art.rewards import ruler_score_group\n",
        "\n",
        "# Test RULER with a simple example\n",
        "base_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You count numbers using numeric symbols.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Count to 10.\"},\n",
        "]\n",
        "\n",
        "good_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\"role\": \"assistant\", \"content\": \"1, 2, 3, 4, 5, 6, 7, 8, 9, 10\"},\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "mediocre_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": \"one, two, three, four, five, six, seven, eight, nine, ten\",\n",
        "        },\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "bad_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\"role\": \"assistant\", \"content\": \"a, b, c, d, e, f, g, h, i, j\"},\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "sample_group = art.TrajectoryGroup(\n",
        "    trajectories=[\n",
        "        good_trajectory,\n",
        "        mediocre_trajectory,\n",
        "        bad_trajectory,\n",
        "    ]\n",
        ")\n",
        "\n",
        "judged_group = await ruler_score_group(sample_group, \"openai/o4-mini\", debug=True)\n",
        "assert judged_group is not None\n",
        "\n",
        "# Display rankings\n",
        "sorted_trajectories = sorted(\n",
        "    judged_group.trajectories, key=lambda t: t.reward, reverse=True\n",
        ")\n",
        "for rank, traj in enumerate(sorted_trajectories, 1):\n",
        "    messages = traj.messages()\n",
        "    print(f\"\\nRank {rank}: Score {traj.reward:.3f}\")\n",
        "    print(f\"  Response: {messages[-1]['content'][:50]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEC3r48iIX6K"
      },
      "source": [
        "<a name=\"Loop\"></a>\n",
        "\n",
        "### Training Loop with LangGraph\n",
        "\n",
        "The training loop is where the magic happens. For each of the steps defined below, the rollout function will be called multiple times in parallel using LangGraph's ReAct agent. Each scenario will produce a trajectory, which will be used to update the model.\n",
        "\n",
        "The `gather` step will wait for all of the trajectories to be generated, then it will use RULER to assign relative scores to each trajectory.\n",
        "\n",
        "Our notebook will then delete all but the most recent checkpoint and train the model on the scored trajectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "from art.utils import iterate_dataset\n",
        "from art.langgraph import wrap_rollout\n",
        "\n",
        "training_config = {\n",
        "    \"groups_per_step\": 2,\n",
        "    \"num_epochs\": 20,\n",
        "    \"rollouts_per_group\": 4,\n",
        "    \"learning_rate\": 1e-5,\n",
        "    \"max_steps\": 20,\n",
        "}\n",
        "\n",
        "# Use iterate_dataset with real training scenarios (similar to train.py)\n",
        "training_iterator = iterate_dataset(\n",
        "    training_scenarios,  # Use real scenarios from Hugging Face\n",
        "    groups_per_step=training_config[\"groups_per_step\"],\n",
        "    num_epochs=training_config[\"num_epochs\"],\n",
        "    initial_step=await model.get_step(),\n",
        ")\n",
        "\n",
        "for batch in training_iterator:\n",
        "    print(\n",
        "        f\"Training step {batch.step}, epoch {batch.epoch}, epoch step {batch.epoch_step}\"\n",
        "    )\n",
        "    print(f\"Batch contains {len(batch.items)} scenarios\")\n",
        "\n",
        "    # Create trajectory groups for this batch (similar to train.py)\n",
        "    groups = []\n",
        "    for scenario in batch.items:\n",
        "        groups.append(\n",
        "            art.TrajectoryGroup(\n",
        "                (\n",
        "                    wrap_rollout(model, rollout)(\n",
        "                        model, EmailScenario(step=batch.step, scenario=scenario)\n",
        "                    )\n",
        "                    for _ in range(training_config[\"rollouts_per_group\"])\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    print(groups[0])\n",
        "    # Gather all trajectory groups\n",
        "    finished_groups = await art.gather_trajectory_groups(\n",
        "        groups,\n",
        "        pbar_desc=\"gather\",\n",
        "        max_exceptions=training_config[\"rollouts_per_group\"] * len(batch.items),\n",
        "    )\n",
        "\n",
        "    judged_groups = []\n",
        "    for group in finished_groups:\n",
        "        # Use RULER to assign relative scores to each trajectory\n",
        "        judged_group = await ruler_score_group(group, \"openai/o4-mini\", debug=True)\n",
        "        judged_groups.append(judged_group)\n",
        "\n",
        "    await model.delete_checkpoints()\n",
        "    await model.train(\n",
        "        judged_groups,\n",
        "        config=art.TrainConfig(learning_rate=training_config[\"learning_rate\"]),\n",
        "        # Lowering the logprob_calculation_chunk_size is a memory saving measure\n",
        "        # to allow longer sequences (up to 8192 tokens) to be processed on a T4.\n",
        "        _config={\"logprob_calculation_chunk_size\": 8},\n",
        "    )\n",
        "\n",
        "    print(f\"Completed training step {batch.step}\")\n",
        "\n",
        "    # Stop after max_steps for demo purposes (adjust as needed)\n",
        "    if batch.step >= training_config[\"max_steps\"]:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wnlNGMlIX6K"
      },
      "source": [
        "### Using the Model\n",
        "\n",
        "LangGraph를 이용해서 Agent가 emails를 검색하고 질문에 대답할 수 있도록 훈련시켰습니다! \n",
        "\n",
        "이제 training loop 밖으로 당신의 모델을 사용할 시간입니다. \n",
        "\n",
        "\n",
        "방금 훈련시킨 모델의 데모를 위해 아래 코드를 확인하세요.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xvofjjLmIX6L"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing the trained LangGraph model with a real scenario...\n",
            "\n",
            "Test scenario ID: 3751\n",
            "Question: What is my schedule for the week of August 28th?\n",
            "Expected answer: Monday--in office; Tuesday--in office; Wednesday--in office; Thursday--vacation; Friday--vacation.\n",
            "Reference message IDs: ['<28971276.1075842953941.JavaMail.evans@thyme>']\n",
            "Inbox: jeff.dasovich@enron.com\n",
            "Query date: 2000-09-20\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_288833/1452329291.py:133: LangGraphDeprecatedSinceV10: create_react_agent has been moved to langchain.agents. Please update your import to 'from langchain.agents import create_agent'. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  react_agent = create_react_agent(chat_model, tools)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading existing database from ./enron_emails.db\n",
            "Loading existing database from ./enron_emails.db\n",
            "Loading existing database from ./enron_emails.db\n",
            "Loading existing database from ./enron_emails.db\n",
            "Loading existing database from ./enron_emails.db\n",
            "LangGraph Agent's trajectory:\n",
            "--------------------\n",
            "[SYSTEM]: \n",
            "You are an email search agent. You are given a user query and a list of tools you can use to search...\n",
            "\n",
            "[USER]: What is my schedule for the week of August 28th?\n",
            "\n",
            "[ASSISTANT]: [{'id': 'chatcmpl-tool-46d34658dbb743978da665a239ef61fe', 'type': 'function', 'function': {'name': 'search_inbox_tool', 'arguments': '{\"keywords\": [\"schedule\", \"August 28th\"]}'}}]\n",
            "\n",
            "[TOOL - unknown_tool]: \n",
            "\n",
            "[ASSISTANT]: [{'id': 'chatcmpl-tool-d0ad457ef20a45388d5cc2324e7cc073', 'type': 'function', 'function': {'name': 'search_inbox_tool', 'arguments': '{\"keywords\": [\"week\", \"August 28th\", \" agenda\", \"plan\"]}'}}]\n",
            "\n",
            "[TOOL - unknown_tool]: \n",
            "\n",
            "[ASSISTANT]: [{'id': 'chatcmpl-tool-36cfeb1bbe404d56aefc5da06d8dcead', 'type': 'function', 'function': {'name': 'search_inbox_tool', 'arguments': '{\"keywords\": [\"September 4th\", \"follow up\", \" August 28th\"]}'}}]\n",
            "\n",
            "[TOOL - unknown_tool]: \n",
            "\n",
            "[ASSISTANT]: [{'id': 'chatcmpl-tool-7b6d91cd15cb4b579ac5a7fe2d557fac', 'type': 'function', 'function': {'name': 'search_inbox_tool', 'arguments': '{\"keywords\": [\"August 28th\", \"meeting\"]}'}}]\n",
            "\n",
            "[TOOL - unknown_tool]: [{\"message_id\": \"<12993450.1075842953497.JavaMail.evans@thyme>\", \"snippet\": \" ... Conference Call <b>meeting</b> for Monday <b>August 28th</b> at 5:00 PM ED\\n>\\n> Bill Hederman asked ... \"}]\n",
            "\n",
            "[ASSISTANT]: [{'id': 'chatcmpl-tool-866f9c393b1c4d899aa632c9d386f451', 'type': 'function', 'function': {'name': 'read_email_tool', 'arguments': '{\"message_id\": \"<12993450.1075842953497.JavaMail.evans@thyme>\"}'}}]\n",
            "\n",
            "[TOOL - unknown_tool]: {\"message_id\": \"<12993450.1075842953497.JavaMail.evans@thyme>\", \"date\": \"2000-08-26 23:23:00\", \"subject\": \"RE: Conference Call meeting for Monday August 28th at 5:00 PM ED\", \"from_address\": \"amosher@a...\n",
            "\n",
            "[ASSISTANT]: Based on the email found, you had a meeting scheduled for August 28th at 5:00 PM EDT. The details are as follows:\n",
            "\n",
            "- Dial in Number: 1-800-832-3242\n",
            "- Option: 3\n",
            "- Option: 1\n",
            "- Meeting ID: 4321\n",
            "\n",
            "If you need more detailed information or if there were other meetings or activities for the week of August 28th, let me know and we can search for them. \n",
            "\n",
            "The message ID for the reference email is `<12993450.1075842953497.JavaMail.evans@thyme>`.\n",
            "\n",
            "Would you like to search for more information on the schedule for the week of August 28th?\n",
            "\n",
            "--------------------------------------------------\n",
            "No final answer provided by the agent\n",
            "\n",
            "Expected Answer: Monday--in office; Tuesday--in office; Wednesday--in office; Thursday--vacation; Friday--vacation.\n",
            "Expected Source IDs: ['<28971276.1075842953941.JavaMail.evans@thyme>']\n",
            "\n",
            "🎉 LangGraph email search agent testing completed!\n",
            "The agent used LangGraph's ReAct pattern with the same inference path as during training.\n"
          ]
        }
      ],
      "source": [
        "#@title Loading/inference code\n",
        "\n",
        "# Test the trained model using the rollout function\n",
        "# This avoids memory issues and uses the same inference path as training\n",
        "\n",
        "print(\"Testing the trained LangGraph model with a real scenario...\\n\")\n",
        "\n",
        "\n",
        "# Use a scenario from our training set\n",
        "test_scenario = training_scenarios[1]\n",
        "\n",
        "print(f\"Test scenario ID: {test_scenario.id}\")\n",
        "print(f\"Question: {test_scenario.question}\")\n",
        "print(f\"Expected answer: {test_scenario.answer}\")\n",
        "print(f\"Reference message IDs: {test_scenario.message_ids}\")\n",
        "print(f\"Inbox: {test_scenario.inbox_address}\")\n",
        "print(f\"Query date: {test_scenario.query_date}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Run the rollout function with the trained model\n",
        "test_email_scenario = EmailScenario.model_validate(\n",
        "    {\"step\": 0, \"scenario\": test_scenario.model_dump()}\n",
        ")\n",
        "result_trajectory = await wrap_rollout(model, rollout)(model, test_email_scenario)\n",
        "\n",
        "print(\"LangGraph Agent's trajectory:\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Display the conversation\n",
        "messages = result_trajectory.messages()\n",
        "for i, msg in enumerate(messages):\n",
        "    role = msg.get(\"role\", \"unknown\")\n",
        "    content = msg.get(\"content\", \"\")\n",
        "    tool_calls = msg.get(\"tool_calls\", [])\n",
        "\n",
        "    if role == \"system\":\n",
        "        print(\n",
        "            f\"[SYSTEM]: {content[:100]}...\"\n",
        "            if len(content) > 100\n",
        "            else f\"[SYSTEM]: {content}\"\n",
        "        )\n",
        "    elif role == \"user\":\n",
        "        print(f\"[USER]: {content}\")\n",
        "    elif role == \"assistant\":\n",
        "        if tool_calls:\n",
        "            print(f\"[ASSISTANT]: {tool_calls}\")\n",
        "        if content:\n",
        "            print(f\"[ASSISTANT]: {content}\")\n",
        "    elif role == \"tool\":\n",
        "        tool_name = msg.get(\"name\", \"unknown_tool\")\n",
        "        print(\n",
        "            f\"[TOOL - {tool_name}]: {content[:200]}...\"\n",
        "            if len(content) > 200\n",
        "            else f\"[TOOL - {tool_name}]: {content}\"\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "print(\"-\" * 50)\n",
        "if result_trajectory.final_answer:\n",
        "    print(f\"Agent's Final Answer: {result_trajectory.final_answer.answer}\")\n",
        "    print(f\"Source IDs Used: {result_trajectory.final_answer.source_ids}\")\n",
        "else:\n",
        "    print(\"No final answer provided by the agent\")\n",
        "\n",
        "print(f\"\\nExpected Answer: {test_scenario.answer}\")\n",
        "print(f\"Expected Source IDs: {test_scenario.message_ids}\")\n",
        "\n",
        "print(\"\\n🎉 LangGraph email search agent testing completed!\")\n",
        "print(\n",
        "    \"The agent used LangGraph's ReAct pattern with the same inference path as during training.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omIgMH2KIX6L"
      },
      "source": [
        "<div class=\"align-center\">\n",
        "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Discord.png\" height=\"50\"></a>\n",
        "<a href=\"https://art.openpipe.ai\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Documentation_pill.png\" height=\"50\"></a>\n",
        "\n",
        "Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [Github](https://github.com/openpipe/art).\n",
        "\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "openpipe",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
