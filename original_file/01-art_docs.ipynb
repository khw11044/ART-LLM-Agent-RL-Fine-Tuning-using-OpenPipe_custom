{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6157b9c7",
   "metadata": {},
   "source": [
    "# ART Docs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03293cea",
   "metadata": {},
   "source": [
    "ART (Agent Reinforcement Trainer) : An open-source framework for LLM reinforcement learning using GRPO\n",
    "\n",
    "\n",
    "GRPO를 사용하는 LLM 강화학습을 위한 open-source 프레임워크인 ART를 이용하여 \n",
    "\n",
    "자신만의 multi-turn agents를 훈련시켜봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e6c575",
   "metadata": {},
   "source": [
    "ART (Agent Reinforcement Trainer)는 Agentic LLMs을 학습시키기 위한 open-source training framework로\n",
    "\n",
    "**경험** 을 통해 **성능과 신뢰성**을 향상시킬 수 있습니다.\n",
    "\n",
    "ART는 GRPO (Group Relative Policy Optimization)과 같은 강화학습 기술들을 편리하게 wrapper로 제공하여 최소한의 훈련 비용으로 모델 성능의 극적인 향상을 이룰 수 있게 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb96b5ab",
   "metadata": {},
   "source": [
    "# Why ART? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0885c8e3",
   "metadata": {},
   "source": [
    "- ART는 기존에 존재하는 애플리케이션에 RL training을 도입하기 위한 편리한 wrappers들을 제공합니다. 이는 개발자의 코드와 상호 작용할 필요가 없는 모듈식 서비스로 훈련 서버를 추상화합니다.\n",
    "\n",
    "- **Train from anywhere.** 노트북에서 ART 클라이언트를 실행하고 ART 서버가 임시 GPU 지원 환경을 시작하도록 하거나 로컬 GPU에서 실행하세요.\n",
    "\n",
    "- W&B, Langfuse, OpenPipe와 같은 호스팅 플랫폼과의 통합은 유연한 관찰성을 제공하고 디버깅을 간소화합니다.\n",
    "\n",
    "- ART는 지능형 기본값을 통해 사용자 정의가 가능합니다. 특정 요구 사항에 맞게 학습 매개변수와 추론 엔진 구성을 구성하거나, 학습 효율성과 안정성을 위해 최적화된 기본값을 활용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2c57a4",
   "metadata": {},
   "source": [
    "# What is RL and when should I use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ba4b62",
   "metadata": {},
   "source": [
    "RL (reinforcement learning)은 AI model 자신의 경험으로부터 학습할 수 있게하는 training 기법 중 하나입니다.\n",
    "\n",
    "RL을 기존 LLM에 적용하는 것은 다음과 같은 것들을 할 수 있습니다: \n",
    "- 전반적인 agent 신뢰성 향상 \n",
    "- QA 또는 생산 과정에서 발견된 특정 실수를 수정\n",
    "- 사용자에게 배포하기 전에 agent 성능의 신뢰성 구축 \n",
    "\n",
    "Examples:\n",
    "- knowledge store로부터 search하고 정보를 추출하기 위한 deep research agent 훈련 \n",
    "- 새로운 training examples를 추가함으로써 모델 behavior에 발생하는 짜증나는 버그 해결\n",
    "- 매번 스크립트를 따르는 초고속 음성 에이전트 만들기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30f83ea",
   "metadata": {},
   "source": [
    "# What do I need in order to use RL?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e059d735",
   "metadata": {},
   "source": [
    "\n",
    "### 필요한것 \n",
    "- ✅ 하나 또는 그 이상의 LLM을 사용할 프로젝트\n",
    "- ✅ LLM이 처리해야 할 시나리오 종류에 대한 지식\n",
    "\n",
    "### 필요하지 않는 것 \n",
    "- ❌ training dataset \n",
    "- ❌ 완성된 reward (보상) 함수 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264f174d",
   "metadata": {},
   "source": [
    "# Tutorial: Implementing OpenPipe ART with Weights & Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0274f1c",
   "metadata": {},
   "source": [
    "해당 튜토리얼에서 우리는 ART를 simple agent task에 적용하고 Weights & Biases (W&B)를 사용하여 training progress를 추적할 것입니다. \n",
    "\n",
    "우리 agent의 task는 다음과 같이 직관적입니다: 간단한 곱셈 문제를 푸는것\n",
    "\n",
    "처음에는, 우리의 language model은 곱셈을 하는데 실수를 할것입니다, 그러나 ART를 이용하면, 이것을 훈련시키며 개선시킬 것입니다. 우리는 W&B Weave와 W&B Models들로 어떻게 지표를 모니터링하는지 알려줄 것이며, trained model을 다루는 방법을 알려줄 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba62c9e",
   "metadata": {},
   "source": [
    "필수조건: W&B 계정이 있어야하고 Python이 설치되어 있어야 합니다. 당신은 당신의 local machone에서 강력한 GPU를 준비할 필요 없습니다 - 우리는 로컬 설정을 사용하여 시연해 보겠습니다. 하지만 GPU가 없다면 SkyPilot 백엔드를 통해 원격 GPU를 사용하도록 ART를 구성할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a66d9",
   "metadata": {},
   "source": [
    "## Step 1 : Install OpenPipe ART and set up W&B\n",
    "\n",
    "먼저, OpenPipe ART 패키지를 설치하세요 그리고 실험 추적으로 위해 weights & Biases에 로그인합니다. \n",
    "당신은 pip를 통해 openpipe-art를 설치할 수 있습니다. 또한, W&B SDK (wandb)를 설치하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2cefafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openpipe-art wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9018329",
   "metadata": {},
   "source": [
    "Python script 또는 notebook에서, W&B를 로그인 하고 초기화 실행을 하세요: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ede788d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhyunew\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/khw/Workspace/llm/openpipe/01.Tutorial/wandb/run-20250919_162325-mifxs6ij</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hyunew/my-agentic-task/runs/mifxs6ij' target=\"_blank\">openpipe-art-demo</a></strong> to <a href='https://wandb.ai/hyunew/my-agentic-task' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hyunew/my-agentic-task' target=\"_blank\">https://wandb.ai/hyunew/my-agentic-task</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hyunew/my-agentic-task/runs/mifxs6ij' target=\"_blank\">https://wandb.ai/hyunew/my-agentic-task/runs/mifxs6ij</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hyunew/my-agentic-task/runs/mifxs6ij?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x76b998f6add0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "\n",
    "wandb.login()  # you'll be prompted to enter your W&B API key (from your W&B account page)\n",
    "wandb.init(project=\"my-agentic-task\", name=\"openpipe-art-demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf479c3",
   "metadata": {},
   "source": [
    "`wandb.init` 호출은 프로젝트(이 경우 \"my-agentic-task\")와 추적을 위한 실행 이름을 설정합니다. 이 작업이 완료되면 로깅하는 모든 지표가 W&B 대시보드에 실시간으로 전송됩니다. (W&B를 사용하지 않으려면 로그인/초기화 단계를 건너뛸 수 있습니다. ART는 이 단계 없이도 작동합니다. 하지만 교육 과정에 대한 통찰력을 얻으려면 이 단계를 적극 권장합니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa7ef7c",
   "metadata": {},
   "source": [
    "## Step 2 : Initialize the ART model and backendNext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903c5d11",
   "metadata": {},
   "source": [
    "우리는 trainable model과 training backend를 설정합니다. \n",
    "\n",
    "파인튜닝 하기위해 base model을 지정해주세요. OpenPipe ART는 많은 Hugging Face-compatible LLMs들을 지원합니다; 여기서 우리는 데모를 위해, (상대적으로 lightweight model인) Qwen 3B instruct model과 같은 smaller model을 사용합니다. 우리는 또한 model 이름을 지어주고 로그인을 위해 W&B project를 지정해줍니다.\n",
    "\n",
    "\n",
    "백엔드의 경우, 로컬 백엔드(사용 가능한 GPU가 있다는 가정 하에)로 시작하겠습니다. 로컬에 GPU가 없다면 SkyPilotBackend를 사용하여 원격 백엔드를 실행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b41c82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khw/miniconda3/envs/openpipe/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-19 16:23:34 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khw/miniconda3/envs/openpipe/lib/python3.11/site-packages/art/__init__.py:10: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  import unsloth  # type: ignore # noqa: F401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 09-19 16:23:41 [__init__.py:235] Automatically detected platform cuda.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: Patching vLLM v1 graph capture\n",
      "Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2025.8.6: Fast Qwen2 patching. Transformers: 4.53.2. vLLM: 0.10.0.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 2. Max memory: 39.394 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen2.5-3b-unsloth-bnb-4bit with actual GPU utilization = 78.07%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 39.39 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 32768. Num Sequences = 320.\n",
      "Unsloth: vLLM's KV Cache can use up to 28.53 GB. Also swap space = 6 GB.\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 09-19 16:24:03 [config.py:1604] Using max model len 32768\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.1.mlp', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.28.mlp', 'model.layers.30.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 09-19 16:24:04 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='unsloth/qwen2.5-3b-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-3b-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen2.5-3b-unsloth-bnb-4bit, num_scheduler_steps=16, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":true,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":320,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 09-19 16:24:06 [cuda.py:398] Using Flash Attention backend.\n",
      "INFO 09-19 16:24:06 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-19 16:24:06 [model_runner.py:1083] Starting to load model unsloth/qwen2.5-3b-unsloth-bnb-4bit...\n",
      "INFO 09-19 16:24:07 [bitsandbytes_loader.py:733] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 09-19 16:24:08 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 09-19 16:24:09 [weight_utils.py:349] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 16.75it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.01s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.01s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-19 16:24:10 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 09-19 16:24:11 [model_runner.py:1115] Model loading took 2.4441 GiB and 3.511720 seconds\n",
      "INFO 09-19 16:24:15 [worker.py:295] Memory profiling takes 3.90 seconds\n",
      "INFO 09-19 16:24:15 [worker.py:295] the current vLLM instance can use total_gpu_memory (39.39GiB) x gpu_memory_utilization (0.78) = 30.75GiB\n",
      "INFO 09-19 16:24:15 [worker.py:295] model weights take 2.44GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 2.69GiB; the rest of the memory reserved for KV Cache is 25.53GiB.\n",
      "INFO 09-19 16:24:16 [executor_base.py:113] # cuda blocks: 46468, # CPU blocks: 10922\n",
      "INFO 09-19 16:24:16 [executor_base.py:118] Maximum concurrency for 32768 tokens per request: 22.69x\n",
      "INFO 09-19 16:24:21 [vllm_utils.py:671] Unsloth: Running patched vLLM v0 `capture_model`.\n",
      "INFO 09-19 16:24:21 [model_runner.py:1385] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 43/43 [00:16<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-19 16:24:37 [model_runner.py:1537] Graph capturing finished in 17 secs, took 5.68 GiB\n",
      "INFO 09-19 16:24:37 [vllm_utils.py:678] Unsloth: Patched vLLM v0 graph capture finished in 17 secs.\n",
      "INFO 09-19 16:24:38 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 27.09 seconds\n",
      "Unsloth: Just some info: will skip parsing ['q_norm', 'post_feedforward_layernorm', 'pre_feedforward_layernorm', 'k_norm']\n",
      "Unsloth: Just some info: will skip parsing ['q_norm', 'post_feedforward_layernorm', 'pre_feedforward_layernorm', 'k_norm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.8.6 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "import art  # import the OpenPipe ART library\n",
    "from art.local import LocalBackend\n",
    "\n",
    "# Create a trainable model instance\n",
    "model = art.TrainableModel(\n",
    "    name=\"agent-001\",                # an arbitrary name for your model (used for logging)\n",
    "    project=\"my-agentic-task\",       # W&B project name for grouping runs\n",
    "    base_model=\"Qwen/Qwen2.5-3B\",    # base model to fine-tune (needs to be a model identifier or path)\n",
    ")\n",
    "\n",
    "\n",
    "# Set up the training backend\n",
    "backend = LocalBackend()  # use local GPU; ensure your machine has a GPU and drivers if using this\n",
    "\n",
    "\n",
    "# If no local GPU, you can use SkyPilotBackend as follows:\n",
    "# from art.skypilot import SkyPilotBackend\n",
    "# backend = await SkyPilotBackend.initialize_cluster(cluster_name=\"art-demo\", gpu=\"A10\")  \n",
    "# (The above would asynchronously provision a remote GPU machine, e.g., with an Nvidia A10 card.)\n",
    "\n",
    "\n",
    "# Register the model with the backend (prepare the backend server)\n",
    "await model.register(backend)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e142a3d",
   "metadata": {},
   "source": [
    "- `TrainableModel` 는 model을 셋업합니다. 우리는 `base_model=\"Qwen/Qwen2.5-3B` 를 예제로 제공합니다. 이는 ART 시스템에서 접근 가능한 모델 체크포인트와 일치해야 합니다 (Hugging Face Hub의 공개 모델인 경우 ART에서 다운로드 할 수 있습니다). 또한 당신이 fine-tuning하기 원하는 다른 모델로 변경할 수 있습니다. \n",
    "\n",
    "- training을 위해 local server process를 런칭할 `LocalBackend` 를 만듭니다. ` await model.register(backend)` 를 호출하면, ART는 백앤드를 시작합니다. (백앤드의 vLLM 서버에서 모델을 로딩하고 훈련할 준비를 합니다). 이 step은 model weights를 메모리에 로드하기 때문에, 초기에 시간을 조금 차지합니다.\n",
    "\n",
    "- Note: `await` 는 `register`가 asynchronous operation 임을 나타냅니다.  top-level `await`를 지원하는 노트북 환경에서는 문제가 없습니다. 일반적인 Python script에서는 `asyncio` event loop 내에서 이를 실행해야 합니다. 간단하게, 이 코드는 notebook 안에 또는 async context 안에 있다고 가정합니다. (만약, script를 사용하면, 이러한 호출을 `async def main()`으로 wrapping하고 `asyncio.run(main())`을 사용합니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9048c580",
   "metadata": {},
   "source": [
    "이제 모델은 사용할 준비가 되었습니다. 추론과 학습을 처리할 수 있는 백엔드에 연결되었습니다. 이제 강화 학습 루프를 시작할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec909a2",
   "metadata": {},
   "source": [
    "## Step 3: Define the task and the reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1305a5fe",
   "metadata": {},
   "source": [
    "해당 튜토리얼에서 우리의 task는 간단합니다: 2개의 수가 주어지면, agent (model)은 이 두 수의 곱셈을 출력합니다. 우리는 해당 모델에 대해 question-answer interaction으로 구조화합니다.\n",
    "\n",
    "예를들어, 우리는 \"What is 12*13?\" 을 제시하면, 대답으로 \"156\" 을 예측합니다. 만약 모델의 대답이 옳다면, 우리는 positive reward를 주고, 만일 틀렸다면, 우리는 negative reward를 줄 것입니다.\n",
    "\n",
    "시간이 지남에 따라, 모델은 더 많은 보상을 받기 위해 출력을 조정해야 합니다(즉, 보이는 숫자 범위에 맞게 올바르게 곱하는 법을 배워야 합니다).\n",
    "\n",
    "우리는 rollout function을 정의해야합니다: 이 function은 agent의 한 episode/interaction을 실행하고 `Trajectory` (메시지 시퀀스 그리고 final reward) 를 return 합니다. \n",
    "ART에서, 일반적으로 `model.openai_clinet()`를 사용하여 모델과 interact할 수 있는 **rollout function**을 작성합니다.\n",
    "\n",
    "아래는 곱셈 작업에 대한 단순화된 rollout function입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16372cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "# Define one scenario rollout for the multiplication task\n",
    "async def rollout(model: art.TrainableModel) -> art.Trajectory:\n",
    "    # Step 1: Prepare a random multiplication problem\n",
    "    a = random.randint(1, 20)\n",
    "    b = random.randint(1, 20)\n",
    "    question = f\"What is {a} * {b}?\"\n",
    "    # We use the model's OpenAI-compatible client for inference\n",
    "    openai_client = model.openai_client()\n",
    "    # Create the conversation message (system prompt can be empty or an instruction)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    # Step 2: Get the model's answer\n",
    "    completion = await openai_client.chat.completions.create(\n",
    "        model=model.name,\n",
    "        messages=messages,\n",
    "        max_tokens=10\n",
    "    )\n",
    "    answer = completion.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "    # Step 3: Calculate reward based on correctness\n",
    "    correct_answer = str(a * b)\n",
    "    if answer == correct_answer:\n",
    "        reward_value = 1.0   # correct answer, positive reward\n",
    "    else:\n",
    "        reward_value = -1.0  # incorrect answer, negative reward\n",
    "\n",
    "\n",
    "    # Step 4: Package into a Trajectory and assign reward\n",
    "    trajectory = art.Trajectory(messages=messages + [{\"role\": \"assistant\", \"content\": answer}])\n",
    "    trajectory.reward = reward_value\n",
    "    return trajectory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50057f16",
   "metadata": {},
   "source": [
    "위 rollout 코드를 파해쳐봅시다.\n",
    "\n",
    "- 곱셈 문제를 풀기 위해 `a`와 `b` 2개의 숫자를 랜덤으로 생성합니다.\n",
    "- 이 숫자들의 곱을 묻는 user message를 구성합니다. (더 복잡한 agent에서는 instruction을 포함한 system message도 추가할 수 있지만, 여기서는 아주 간단하게 구성합니다)\n",
    "- 우리의 모델에서 `openai_client`를 가져오고, `chat.completions.create` 메서드를 사용하여 모델의 응답을 가져옵니다. `model=model.name`을 지정하여 요청이 백엔드에 등록한 fine-tuning 인스턴스로 전달되도록 합니다.\n",
    "- 모델의 대답이 캡쳐됩니다. formatting 문제를 방지하기 위해 해당 답변을 제거합니다.\n",
    "- 그런다음 reward를 결정합니다: 만일 대답이 정답이면, reward = +1, 아니면, -1로 매핑합니다. (우리는 간단한 reward 스키마를 사용합니다; 더 복잡한 task는 더 복잡한 scoring이 필요합니다.)\n",
    "- 우리는 `art.Trajectory` object를 생성하여 interaction을 기록합니다. 우리는 user question과 assistant's answer를 모두 messages에 포함합니다. 그런다음, 우리는 trajectory에 reward를 할당합니다.\n",
    "- trajectory가 return되어, 해당 환경에서 agent의 한 rollout이 캡슐화됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a009031",
   "metadata": {},
   "source": [
    "## Step 4: Run training iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe41b7b",
   "metadata": {},
   "source": [
    "정의된 rollout function과 함꼐, 여러 iterations를 통해 모델을 훈련시킬 수 있습니다.\n",
    "강화 학습의 관점에서 보면, 우리는 일련의 에피소드를 진행하고, 각 에피소드 배치가 끝날 때마다 모델을 업데이트합니다.\n",
    "\n",
    "데모를 위해, 우리는 50번의 training setip을 실행합니다. 각 스텝에서, 우리는 여러 곱셈 문제들을 agent에게 제공합니다 (update 전에 충분한 경험을 수집하기 위해)\n",
    "\n",
    "\n",
    "OpenPipe ART를 사용하여, model을 training 하는 것은 \n",
    "\n",
    "\n",
    "OpenPipe ART를 사용하면 롤아웃에서 train method를 호출하는 것만큼 쉽게 모델을 training 할 수 있습니다.\n",
    "예를 들어, ART를 사용하면 여러 롤아웃을 그룹화하고 내부적으로 training을 처리할 수 있습니다.\n",
    "\n",
    "아래는 training에 대한 pseudo code 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b92eb200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31a76d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_443543/371425098.py:8: RuntimeWarning: coroutine 'rollout' was never awaited\n",
      "  trajectories = [rollout(model) for _ in range(rollouts_per_step)]\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/home/khw/miniconda3/envs/openpipe/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_chat.py:204: RuntimeWarning: coroutine 'AsyncMultiModalItemTracker.all_mm_data' was never awaited\n",
      "  return self.create_error_response(f\"{e} {e.__cause__}\")\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'object': 'error', 'message': 'As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one. None', 'type': 'BadRequestError', 'param': None, 'code': 400}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m trajectories = [rollout(model) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(rollouts_per_step)]\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Wait for all the asynchronous rollouts to finish\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m trajectories = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*trajectories)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# (ART will automatically use these trajectories to train the model via GRPO)\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# After this point, the model's parameters (LoRA weights) are updated on the backend.\u001b[39;00m\n\u001b[32m     15\u001b[39m \n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Log the mean reward of this batch for monitoring\u001b[39;00m\n\u001b[32m     17\u001b[39m avg_reward = \u001b[38;5;28msum\u001b[39m(t.reward \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m trajectories) / \u001b[38;5;28mlen\u001b[39m(trajectories)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/openpipe/lib/python3.11/asyncio/tasks.py:349\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    350\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    351\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    352\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/openpipe/lib/python3.11/asyncio/tasks.py:277\u001b[39m, in \u001b[36mTask.__step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    275\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    276\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    279\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mrollout\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m     14\u001b[39m messages = [\n\u001b[32m     15\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: question}\n\u001b[32m     16\u001b[39m ]\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Step 2: Get the model's answer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m completion = \u001b[38;5;28;01mawait\u001b[39;00m openai_client.chat.completions.create(\n\u001b[32m     19\u001b[39m     model=model.name,\n\u001b[32m     20\u001b[39m     messages=messages,\n\u001b[32m     21\u001b[39m     max_tokens=\u001b[32m10\u001b[39m\n\u001b[32m     22\u001b[39m )\n\u001b[32m     23\u001b[39m answer = completion.choices[\u001b[32m0\u001b[39m].message.content.strip()\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Step 3: Calculate reward based on correctness\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/openpipe/lib/python3.11/site-packages/art/openai.py:34\u001b[39m, in \u001b[36mpatch_openai.<locals>.create_patched\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     33\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m] = {\u001b[33m\"\u001b[39m\u001b[33minclude_usage\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m return_value = \u001b[38;5;28;01mawait\u001b[39;00m create(*args, **kwargs)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_value, AsyncIterator):\n\u001b[32m     36\u001b[39m     report_usage(return_value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/openpipe/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:2028\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1985\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1986\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1987\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2025\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2026\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2027\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2028\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2029\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2030\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2031\u001b[39m             {\n\u001b[32m   2032\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2033\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2034\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2035\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2036\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2037\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2038\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2039\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2040\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2041\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2042\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2043\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2044\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2045\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2046\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2047\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2048\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2049\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2050\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2051\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2052\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2053\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2054\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2055\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2056\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2057\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2058\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2059\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2060\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2061\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2062\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2063\u001b[39m             },\n\u001b[32m   2064\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2065\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2066\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2067\u001b[39m         ),\n\u001b[32m   2068\u001b[39m         options=make_request_options(\n\u001b[32m   2069\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2070\u001b[39m         ),\n\u001b[32m   2071\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2072\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2073\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2074\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/openpipe/lib/python3.11/site-packages/openai/_base_client.py:1784\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1770\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1772\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1779\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1780\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1781\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1782\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1783\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/openpipe/lib/python3.11/site-packages/openai/_base_client.py:1584\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1581\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1583\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1584\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1586\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1588\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'object': 'error', 'message': 'As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one. None', 'type': 'BadRequestError', 'param': None, 'code': 400}"
     ]
    }
   ],
   "source": [
    "# Train the model for a certain number of steps\n",
    "training_steps = 50\n",
    "rollouts_per_step = 8  # how many problems to attempt per step (parallel rollouts)\n",
    "\n",
    "\n",
    "for step in range(training_steps):\n",
    "    # Collect trajectories from multiple parallel rollouts\n",
    "    trajectories = [rollout(model) for _ in range(rollouts_per_step)]\n",
    "    # Wait for all the asynchronous rollouts to finish\n",
    "    trajectories = await asyncio.gather(*trajectories)\n",
    "\n",
    "\n",
    "    # (ART will automatically use these trajectories to train the model via GRPO)\n",
    "    # After this point, the model's parameters (LoRA weights) are updated on the backend.\n",
    "    \n",
    "    # Log the mean reward of this batch for monitoring\n",
    "    avg_reward = sum(t.reward for t in trajectories) / len(trajectories)\n",
    "    wandb.log({\"step\": step, \"average_reward\": avg_reward})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2decfb",
   "metadata": {},
   "source": [
    "위 코드에서, 우린는 `rollout(model)` 을 동시에 8번 실행시켜 경험을 수집합니다. 그런 다음 (async context인 경우 `asyncio.gather`를 사용하여) 모든 항목을 await 하여 trajectories results의 list를 가져옵니다. 이런 trajectories는 agent의 interactons과 reward를 포함합니다. 이것들이 수집되면, ART의 backend는 내부적으로 training이 업데이트 될것입니다. (ART는 내부적으로 trajectories를 그룹화하고 GRPO 최적화 단계를 실행하여 모델의 policy를 개선합니다.)\n",
    "\n",
    "우리는 또한 rollouts의 batch에 대해 reward 평균을 W&B에 로깅합니다. 이것은 training progress와 같은 것을 보기에 유용한 지표입니다.\n",
    "\n",
    "\n",
    "또한 해당 rollouts의 batch에 대한 평균 reward를 W&B에 로깅합니다. 이는 관찰하기 유용한 지표입니다. 훈련이 진행됨에 따라 Agent가 성공적으로 학습하고 있다면 평균 reward가 1.0에 가까워질 것으로 예상할 수 있습니다.\n",
    "우리의 곱셈 task 예시에서, 평균 reward 1.0은 에이전트가 해당 배치에서 모든 답을 올바르게 맞췄다는 것을 의미합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23f5f7c",
   "metadata": {},
   "source": [
    "## Step 5: Monitor training with Weave and W&B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74040ab",
   "metadata": {},
   "source": [
    "training loop를 실행하면, 당신은 실시간으로 지표를 관측하기 위해 Weight & Biases project page로 이동 할 수 있습니다. 당신은 각 step별로 로깅된 training curve를 포함해 `average_reward` 지표를 볼 수 있습니다. 만일 당신이 ART에서 모델의 `project` 와 `name` 을 제대로 설정하였다면, ART는 아마 (loss 또는 policy entropy와 같은) 추가적인 지표를 자동으로 W&B에 로깅할 것입니다. \n",
    "\n",
    "\n",
    "이때 W&B Weave가 유용합니다. Weave를 사용하면 에이전트의 성능을 심층적으로 분석할 수 있는 interactive 대시보드를 만들 수 있습니다. 예를 들어, 단순히 reward curve를 보여주는 것뿐 아니라, 질문과 답변 샘플을 함께 표시하여 모델 출력이 어떻게 개선되는지를 확인할 수 있는 Weave 패널을 구축할 수 있습니다. 또, 곱셈 문제와 에이전트가 서로 다른 학습 단계에서 낸 답변을 표 형태로 나열하는 것도 가능하며, 이는 정성적 평가에 매우 유용합니다. Weave의 강점은 플롯, 텍스트, 심지어 모델 쿼리까지 한 공간에 결합할 수 있다는 점으로, AI 애플리케이션 분석을 위한 유연한 툴킷입니다.\n",
    "\n",
    "또한 Weights & Biases에는 Models(모델 레지스트리 및 아티팩트)라는 기능도 있습니다. 학습이 끝나면, 예를 들어 ART가 학습한 LoRA 어댑터와 같은 파인튜닝된 모델을 저장하고 싶을 것입니다. W&B를 사용하면 이러한 결과물을 버저닝할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394290cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training is done, save the LoRA weights \n",
    "final_lora_dir = \"./.art/models\"  # hypothetical path where ART stored LoRA checkpoints\n",
    "# Log the fine-tuned model as a W&B artifact for versioning\n",
    "artifact = wandb.Artifact(name=\"multiplication-agent-lora\", type=\"model\")\n",
    "artifact.add_dir(final_lora_dir)\n",
    "wandb.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ed7e72",
   "metadata": {},
   "source": [
    "아티팩트를 기록함으로써, 학습된 에이전트의 모델 가중치를 W&B에 영구적으로 보관할 수 있고, 이후에 이를 다시 불러오거나, 비교하거나, 심지어 배포할 수도 있습니다. W&B Models(모델 레지스트리)는 이러한 아티팩트와 그 계보(lineage)를 추적할 수 있어, 어떤 학습 실행(run)에서 어떤 모델이 생성되었는지를 명확히 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315388c8",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate the trained agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ceda0",
   "metadata": {},
   "source": [
    "50번의 steps를 훈련시킨 이후라면, 이제 agent가 얼마나 잘 학습 되었는지 test 할 시간입니다.\n",
    "\n",
    "`model.openai_client()`를 다시 사용하면 모델이 곱셈 문제에 답하고 정답인지 확인할 수 있습니다.\n",
    "\n",
    "이상적으로는 이 모델이 처음보다 이 작업에서 훨씬 더 안정적이라는 것을 알게 될 것입니다.\n",
    "\n",
    "훈련 후, 빠른 테스트를 위해, 아래 몇가지 샘플 쿼리를 시도하세요:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3941825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\"What is 7  8?\", \"What is 15  14?\", \"What is 3 * 19?\"]\n",
    "for q in test_questions:\n",
    "    completion = await openai_client.chat.completions.create(\n",
    "        model=model.name,\n",
    "        messages=[{\"role\": \"user\", \"content\": q}]\n",
    "    )\n",
    "    answer = completion.choices[0].message.content.strip()\n",
    "    print(f\"Q: {q} -> A: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb87449",
   "metadata": {},
   "source": [
    "학습이 제대로 이루어졌다면, 위의 쿼리에 대한 답변이 정확히 나와야 합니다(예: `56, 210, 57`). 만약 일부가 틀리거나 정확도를 더 높이고 싶다면, 학습 스텝을 더 늘리거나 전략을 조정할 수 있습니다(예: 숫자의 범위를 넓히거나 보상 체계를 바꾸는 방법).\n",
    "\n",
    "다른 활용 사례: 지금 살펴본 예시는 매우 단순하지만, OpenPipe ART는 다양한 에이전트 작업에 적용할 수 있습니다. 수학 문제 대신 “환경(environment)”을 `게임(예: 2048, 틱택토—ART로 이미 시연된 바 있음)`, `지식 검색 작업(에이전트가 도구를 사용해 정보를 찾고 요약)`, 혹은 `대화 워크플로우(에이전트가 긴 대화 속에서 지시를 따라야 하는 경우)`로 설정할 수도 있습니다. 과정은 대부분 동일합니다:\n",
    "\n",
    "1. 에이전트가 처리해야 할 시나리오를 정의합니다.\n",
    "\n",
    "2. ART 클라이언트를 통해 해당 시나리오에서 에이전트와 상호작용하고, 마지막에 보상을 부여하는 rollout 함수를 작성합니다.\n",
    "\n",
    "3. 여러 번의 rollout을 실행해 ART가 모델의 동작을 최적화하도록 합니다.\n",
    "\n",
    "보상 함수와 시나리오를 조정함으로써, 게임 플레이부터 대화 속 사실성이나 안전성 가이드라인 준수까지 다양한 기술을 에이전트에게 학습시킬 수 있습니다.\n",
    "\n",
    "이 모든 과정에서 W&B는 실험 추적에 매우 유용합니다. 예를 들어, 다른 보상 함수를 사용했을 때 학습이 더 빨라졌는지 비교할 수 있고, 성능을 시각화하며, 최상의 모델을 관리할 수 있습니다. OpenPipe ART와 W&B의 통합 덕분에 하이퍼파라미터를 튜닝하거나 에이전트의 동작을 디버깅할 때 매우 풍부한 인사이트를 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f88b7a",
   "metadata": {},
   "source": [
    "## Step 7: 모델 저장 및 허깅페이스에 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d49bbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f16551b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e68bbde",
   "metadata": {},
   "source": [
    "# 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33d3f3d",
   "metadata": {},
   "source": [
    "이 글에서는 OpenPipe ART를 기초적인 목적부터 실제 구현까지 깊이 있게 살펴보았습니다. 핵심 내용을 요약하면 다음과 같습니다. OpenPipe ART는 언어 모델 에이전트에 강화학습을 손쉽게 적용할 수 있게 해주는 강력한 프레임워크입니다. 이는 전통적인 강화학습 프레임워크의 여러 한계를 해결하는데, 다중 턴 에이전트 워크플로우를 지원하고, 클라이언트-서버 아키텍처를 통한 GPU 활용 극대화, OpenAI 스타일의 인터페이스를 통한 기존 코드베이스와의 손쉬운 통합이 그 예입니다. \n",
    "실무자에게 이는 곧, 성능이 부족하거나 실수를 반복하는 에이전트를 단순한 정적 파인튜닝이 아니라 경험을 통해 개선할 수 있음을 의미합니다. 그 결과로 신뢰성 향상(과거의 실수를 회피), 더 나은 과제 수행, 배포된 시스템에서의 지속 학습 가능성이라는 이점이 주어집니다.\n",
    "\n",
    "우리는 ART 학습 루프를 설정하는 방법을 시연하고, Weights & Biases(W&B) 통합을 강조했습니다. W&B의 Weave와 Models를 활용하면 학습 진행 상황을 실시간으로 모니터링하고 학습된 모델을 관리할 수 있어, 실험 과정이 크게 향상됩니다. 단순한 곱셈 문제 학습 예시는 복잡한 작업을 다룰 때의 접근 방식을 단순화해 보여주는 것이었으며, 동일한 원리가 복잡한 의사결정이나 도구 사용과 같은 작업을 학습하는 데에도 적용됩니다. 즉, 작업을 정의하고, 보상으로 피드백을 제공하며, 나머지는 ART에 맡기면 됩니다.\n",
    "\n",
    "앞으로를 내다보면, OpenPipe ART는 활발히 진화하고 있는 프로젝트입니다. 향후 발전 방향은 다음과 같을 수 있습니다:\n",
    "\n",
    "강화된 보상 모델링: 현재는 보상을 직접 정의하거나 간단한 휴리스틱을 사용하지만, 앞으로는 학습된 보상 모델이나 인간 피드백(RLHF와 유사)을 직접 활용하는 고급 기법이 통합될 수 있습니다. 실제로 ART는 이미 RULER라는 기능을 도입해 LLM을 판정자로 활용, 경로를 점수화하고 수작업 보상 함수의 필요성을 크게 줄였습니다. 이러한 기능은 에이전트의 “성공” 기준 정의를 더 쉽게 만들 것입니다.\n",
    "\n",
    "더 폭넓은 모델 지원과 확장성: 대규모 및 특수화 모델이 등장함에 따라 ART는 이를 지원하는 방향으로 확장될 것입니다. 프런트엔드와 백엔드의 분리 덕분에 분산 학습이나 대규모 모델의 하드웨어 샤딩까지 가능성이 열려 있으며, 새로운 하드웨어 최적화나 GRPO 알고리즘 효율성 개선도 기대할 수 있습니다.\n",
    "\n",
    "프로덕션 통합: 현재 ART는 연구 및 프로토타이핑에 적합하지만, 장기적으로는 실제 서비스에 통합될 도구들이 등장할 수 있습니다. 예를 들어, 실제 사용자 상호작용 데이터를 기반으로 자동 학습하여(적절한 안전장치와 함께) 배포된 에이전트가 지속적으로 개선되는 방식입니다. 미래에는 이러한 지속 학습을 안전하고 간단하게 구현할 수 있는 기능이 추가될 가능성이 있습니다.\n",
    "\n",
    "커뮤니티 기여와 생태계: 오픈소스 프로젝트인 만큼, ART의 발전은 사용자 커뮤니티의 기여에 크게 의존합니다. LangChain 같은 에이전트 오케스트레이션 라이브러리와의 통합, 커뮤니티 기반 시나리오 라이브러리, 더 많은 예제들이 생겨날 수 있습니다. 또한 W&B 생태계와의 결합을 통해 공개 대시보드, 리포트, 모범 사례 공유 등이 활발히 이루어질 것입니다.\n",
    "\n",
    "OpenPipe ART는 대규모 언어 모델 기반 AI 에이전트에 강화학습을 실용적으로 적용하는 데 있어 중요한 진전을 보여줍니다. 개발자들은 정적인 모델 성능을 넘어, 시행착오를 통한 지속적인 행동 개선을 가능하게 할 수 있습니다. 자율 에이전트나 복잡한 챗봇을 만들고 있는데 기대만큼 강건하거나 정확하지 않다면, ART를 고려해보십시오. 최소한의 수정만으로 에이전트에 학습 루프를 내장할 수 있고, 점차 개선되는 모습을 확인할 수 있습니다. 또한 W&B 같은 도구로 학습 과정을 추적하면, 학습 과정을 완전히 투명하게 파악할 수 있습니다.\n",
    "\n",
    "OpenPipe ART의 미래는 매우 밝습니다. 오픈소스 개발, 성장하는 사용자 커뮤니티, MLOps 도구와의 통합이 맞물려, 이 프레임워크는 앞으로 표준적인 에이전트 학습 접근 방식으로 자리 잡을 수 있습니다. 이는 자율적인 LLM 기반 에이전트가 할 수 있는 일의 범위를 계속해서 확장시킬 것입니다.\n",
    "\n",
    "읽어주셔서 감사합니다. 즐거운 학습 되시길 바랍니다! 더 깊이 탐구하고 싶으시다면, 관련 예제를 확인하고 OpenPipe 커뮤니티에 참여해 보시길 권장합니다. 게임 플레이 에이전트든, 사용자 상호작용을 통해 학습하는 AI 어시스턴트든, OpenPipe ART와 W&B는 더 높은 성능을 달성하는 데 필요한 피드백 루프를 제공할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dc2e2e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openpipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
